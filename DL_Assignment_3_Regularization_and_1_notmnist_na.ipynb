{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL Assignment 3 Regularization and 1_notmnist_na.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "lrAMsvnGC5xJ",
        "5hIbr52I7Z7U",
        "4riXK3IoHgx6",
        "vUdbskYE2d87",
        "cYznx5jUwzoO",
        "puDUTe6t6USl",
        "gE_cRAQB33lk"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Khamphang/test-repositry/blob/master/DL_Assignment_3_Regularization_and_1_notmnist_na.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrAMsvnGC5xJ",
        "colab_type": "text"
      },
      "source": [
        "# Assignment 1: notmnist"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4nQryjitGE4g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "2a7cebac-ea61-4d67-9d67-45894b1e498e"
      },
      "source": [
        "### 200525 ເຮົາຈະ downgrade ຂອງ tensorflow ຈາກ 2.x ເປັນ 1.x ເພື່ອໃຊ້ກັບ Assignment 3 Regularization\n",
        "%tensorflow_version 1.x\n",
        "\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "print(tf.keras.__version__)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n",
            "1.15.2\n",
            "2.2.4-tf\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "colab_type": "code",
        "deletable": true,
        "editable": true,
        "id": "apJbCsBHl-2A",
        "colab": {}
      },
      "source": [
        "# These are all the modules we'll be using later. Make sure you can import them\n",
        "# before proceeding further.\n",
        "### ຄຳຕອບຂອງ Assignment ນີ້: http://www.ritchieng.com/machine-learning/deep-learning/tensorflow/notmnist/\n",
        "\n",
        "from __future__ import print_function\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "import tarfile\n",
        "###       ໂຕ import ລຸ່ມນີ້ແມ່ນ ນາເພີ່ມໃສ່        ###\n",
        "import imageio\n",
        "\n",
        "from IPython.display import display, Image\n",
        "from scipy import ndimage\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from six.moves.urllib.request import urlretrieve\n",
        "from six.moves import cPickle as pickle\n",
        "\n",
        "###   ຖ້າໜ້າແຖວມີ ### ແມ່ນແຖວນັ້ນນາເພີ່ມໃສ່  \n",
        "np.set_printoptions(precision=2)\n",
        "###import pdb\n",
        "###  import pdb; pdb.set_trace()   ###   ນາ    ####\n",
        "###   ນາເພີ່ມໃສ່     ####\n",
        "\n",
        "# Config the matplotlib backend as plotting inline in IPython\n",
        "%matplotlib inline"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "deletable": true,
        "editable": true,
        "id": "5hIbr52I7Z7U"
      },
      "source": [
        "Deep Learning\n",
        "=============\n",
        "\n",
        "Assignment 1\n",
        "------------\n",
        "\n",
        "The objective of this assignment is to learn about simple data curation practices, and familiarize you with some of the data we'll be reusing later.\n",
        "\n",
        "This notebook uses the [notMNIST](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html) dataset to be used with python experiments. This dataset is designed to look like the classic [MNIST](http://yann.lecun.com/exdb/mnist/) dataset, while looking a little more like real data: it's a harder task, and the data is a lot less 'clean' than MNIST."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "deletable": true,
        "editable": true,
        "id": "jNWGtZaXn-5j"
      },
      "source": [
        "First, we'll download the dataset to our local machine. The data consists of characters rendered in a variety of fonts on a 28x28 image. The labels are limited to 'A' through 'J' (10 classes). The training set has about 500k and the testset 19000 labelled examples. Given these sizes, it should be possible to train models quickly on any machine. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tuFKE5MYrQKa",
        "colab_type": "text"
      },
      "source": [
        "ບໍ່ຮູ້ວ່າຫຼັງຈາກ download ແລ້ວມັນໄປຢູ່ບ່ອນໃດຂອງ notebook ນີ້ ຖ້າຈະເປີດດ້ວຍ window explorer ມັນຈະຢູ່ບ່ອນໃດນໍ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "deletable": true,
        "editable": true,
        "id": "cC3p0oEyF8QT"
      },
      "source": [
        "Extract the dataset from the compressed .tar.gz file.\n",
        "This should give you a set of directories, labelled A through J."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "colab_type": "code",
        "deletable": true,
        "editable": true,
        "id": "EYRJ4ICW6-da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "e0c1f3c0-45c0-4bcf-ee64-d35392abca37"
      },
      "source": [
        "url = 'http://commondatastorage.googleapis.com/books1000/'\n",
        "last_percent_reported = None\n",
        "data_root = '.' # Change me to store data elsewhere\n",
        "# data_root = u'C:' # Change me to store data elsewhere\n",
        "\n",
        "def download_progress_hook(count, blockSize, totalSize):\n",
        "  \"\"\"A hook to report the progress of a download. This is mostly intended for users with\n",
        "  slow internet connections. Reports every 5% change in download progress.\n",
        "  \"\"\"\n",
        "  global last_percent_reported\n",
        "  percent = int(count * blockSize * 100 / totalSize)\n",
        "\n",
        "  if last_percent_reported != percent:\n",
        "    if percent % 5 == 0:\n",
        "      sys.stdout.write(\"%s%%\" % percent)\n",
        "      sys.stdout.flush()\n",
        "    else:\n",
        "      sys.stdout.write(\".\")\n",
        "      sys.stdout.flush()\n",
        "      \n",
        "    last_percent_reported = percent\n",
        "        \n",
        "def maybe_download(filename, expected_bytes, force=False):\n",
        "## Na ## def maybe_download(filename, expected_bytes, force=True):\n",
        "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
        "  dest_filename = os.path.join(data_root, filename)\n",
        "  if force or not os.path.exists(dest_filename):\n",
        "    print('Attempting to download:', filename) \n",
        "    filename, _ = urlretrieve(url + filename, dest_filename, reporthook=download_progress_hook)\n",
        "    print('\\nDownload Complete!')\n",
        "  statinfo = os.stat(dest_filename)\n",
        "  if statinfo.st_size == expected_bytes:\n",
        "    print('Found and verified', dest_filename)\n",
        "  else:\n",
        "    raise Exception(\n",
        "      'Failed to verify ' + dest_filename + '. Can you get to it with a browser?')\n",
        "  return dest_filename\n",
        "\n",
        "train_filename = maybe_download('notMNIST_large.tar.gz', 247336696)\n",
        "test_filename = maybe_download('notMNIST_small.tar.gz', 8458043)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Attempting to download: notMNIST_large.tar.gz\n",
            "0%....5%....10%....15%....20%....25%....30%....35%....40%....45%....50%....55%....60%....65%....70%....75%....80%....85%....90%....95%....100%\n",
            "Download Complete!\n",
            "Found and verified ./notMNIST_large.tar.gz\n",
            "Attempting to download: notMNIST_small.tar.gz\n",
            "0%....5%....10%....15%....20%....25%....30%....35%....40%....45%....50%....55%....60%....65%....70%....75%....80%....85%....90%....95%....100%\n",
            "Download Complete!\n",
            "Found and verified ./notMNIST_small.tar.gz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "colab_type": "code",
        "deletable": true,
        "editable": true,
        "id": "H8CBE-WZ8nmj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "f2dcb06d-81a1-44da-f299-4fac158f1d81"
      },
      "source": [
        "num_classes = 10\n",
        "np.random.seed(133)   ## 133 ໝາຍເຖິງຫຍັງນໍ ??? An array of random numbers in the [0.0, 1.0]\n",
        "\n",
        "def maybe_extract(filename, force=False):\n",
        "  root = os.path.splitext(os.path.splitext(filename)[0])[0]  # remove .tar.gz\n",
        "  if os.path.isdir(root) and not force:\n",
        "    # You may override by setting force=True.\n",
        "    print('%s already present - Skipping extraction of %s.' % (root, filename))\n",
        "  else:\n",
        "    print('Extracting data for %s. This may take a while. Please wait.' % root)\n",
        "    tar = tarfile.open(filename)\n",
        "    sys.stdout.flush()\n",
        "    tar.extractall(data_root)\n",
        "    tar.close()\n",
        "  data_folders = [\n",
        "    os.path.join(root, d) for d in sorted(os.listdir(root))\n",
        "    if os.path.isdir(os.path.join(root, d))]\n",
        "  if len(data_folders) != num_classes:\n",
        "    raise Exception(\n",
        "      'Expected %d folders, one per class. Found %d instead.' % (\n",
        "        num_classes, len(data_folders)))         # The method len() returns the \n",
        "  print(data_folders)                            # number of elements in the list\n",
        "  return data_folders\n",
        "  \n",
        "train_folders = maybe_extract(train_filename)    # train_folders ເປັນ list ເກັບທີ່\n",
        "test_folders = maybe_extract(test_filename)      # ຢູ່ຂອງຮູບຕາມແຕ່ລະໂຕອັກສອນ ເຊັ່ນ:\n",
        "                                                 # './notMNIST_large/D'"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting data for ./notMNIST_large. This may take a while. Please wait.\n",
            "['./notMNIST_large/A', './notMNIST_large/B', './notMNIST_large/C', './notMNIST_large/D', './notMNIST_large/E', './notMNIST_large/F', './notMNIST_large/G', './notMNIST_large/H', './notMNIST_large/I', './notMNIST_large/J']\n",
            "Extracting data for ./notMNIST_small. This may take a while. Please wait.\n",
            "['./notMNIST_small/A', './notMNIST_small/B', './notMNIST_small/C', './notMNIST_small/D', './notMNIST_small/E', './notMNIST_small/F', './notMNIST_small/G', './notMNIST_small/H', './notMNIST_small/I', './notMNIST_small/J']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "deletable": true,
        "editable": true,
        "id": "4riXK3IoHgx6"
      },
      "source": [
        "---\n",
        "Problem 1\n",
        "---------\n",
        "\n",
        "Let's take a peek at some of the data to make sure it looks sensible. Each exemplar should be an image of a character A through J rendered in a different font. Display a sample of the images that we just downloaded. Hint: you can use the package IPython.display.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "lHQj3MEH5GIR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "061a91ef-0def-4367-85b9-3adeb1ea7733"
      },
      "source": [
        "image_size = 28  # Pixel width and height.\n",
        "pixel_depth = 255.0  # Number of levels per pixel.\n",
        "\n",
        "# We get all file names in I directory\n",
        "fn = os.listdir(\"notMNIST_small/I/\")\n",
        "\n",
        "# Display first 1 images of I directory\n",
        "for file in fn[0:2]:\n",
        "    path = 'notMNIST_small/I/' + file\n",
        "\n",
        "    image_data_Na = imageio.imread(path).astype(float)\n",
        "\n",
        "    print(image_data_Na)\n",
        "\n",
        "    display(Image(path))\n",
        "\n",
        "    image_data_Na = (image_data_Na - pixel_depth / 2) / pixel_depth  # ຂັ້ນຕອນນີ້ຄິດວ່າແມ່ນ່ປ່ຽນຮູບຕົວເລກແຕ່ລະພິກເຊວຮູບເປັນຮູບແບບ zero mean and standard deviation ~0.5     \n",
        "                   # to make training easier down the road ເພື່ອງ້າຍໃນການປະຜົນຂອງຄອມ\n",
        "\n",
        "    print(image_data_Na)\n",
        "\n",
        "    display(Image(path))\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255.\n",
            "  255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255.]\n",
            " [255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255.\n",
            "  255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255.]\n",
            " [255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255.\n",
            "  255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255.]\n",
            " [255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255.\n",
            "  255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255.]\n",
            " [255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255.\n",
            "  255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255.]\n",
            " [255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255.\n",
            "  255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255.]\n",
            " [255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255.\n",
            "  255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255.]\n",
            " [255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255.\n",
            "  255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255.]\n",
            " [255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255.\n",
            "  255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255.]\n",
            " [255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255.\n",
            "  255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255.]\n",
            " [255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255.\n",
            "  255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255.]\n",
            " [255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255.\n",
            "  255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255.]\n",
            " [255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255.\n",
            "  255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255.]\n",
            " [255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255.\n",
            "  255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255.]\n",
            " [255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255.\n",
            "  255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255.]\n",
            " [255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255.\n",
            "  255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255.]\n",
            " [255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255.\n",
            "  255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255.]\n",
            " [255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255.\n",
            "  255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255.]\n",
            " [255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255.\n",
            "  255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255.]\n",
            " [255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255.\n",
            "  255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255.]\n",
            " [255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255.\n",
            "  255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255.]\n",
            " [255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255.\n",
            "  255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255.]\n",
            " [255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255.\n",
            "  255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255.]\n",
            " [255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255.\n",
            "  255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255.]\n",
            " [255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255.\n",
            "  255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255.]\n",
            " [255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255.\n",
            "  255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255.]\n",
            " [255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255.\n",
            "  255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255.]\n",
            " [255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255.\n",
            "  255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255.]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAFklEQVR4nGP8z4AbMOGRG5UclRxhkgCD/gE3CChK/QAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "[[0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
            "  0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]\n",
            " [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
            "  0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]\n",
            " [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
            "  0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]\n",
            " [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
            "  0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]\n",
            " [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
            "  0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]\n",
            " [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
            "  0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]\n",
            " [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
            "  0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]\n",
            " [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
            "  0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]\n",
            " [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
            "  0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]\n",
            " [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
            "  0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]\n",
            " [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
            "  0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]\n",
            " [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
            "  0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]\n",
            " [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
            "  0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]\n",
            " [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
            "  0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]\n",
            " [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
            "  0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]\n",
            " [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
            "  0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]\n",
            " [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
            "  0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]\n",
            " [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
            "  0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]\n",
            " [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
            "  0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]\n",
            " [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
            "  0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]\n",
            " [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
            "  0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]\n",
            " [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
            "  0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]\n",
            " [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
            "  0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]\n",
            " [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
            "  0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]\n",
            " [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
            "  0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]\n",
            " [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
            "  0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]\n",
            " [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
            "  0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]\n",
            " [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
            "  0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAFklEQVR4nGP8z4AbMOGRG5UclRxhkgCD/gE3CChK/QAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "[[124. 236. 255. 255. 255. 254. 253. 255. 255. 255. 255. 255. 255. 255.\n",
            "  255. 255. 255. 255. 255. 255. 255. 253. 254. 255. 255. 255. 240. 149.]\n",
            " [ 79. 165. 187. 204. 231. 250. 255. 255. 255. 255. 255. 255. 255. 255.\n",
            "  255. 255. 255. 255. 255. 255. 255. 255. 251. 233. 207. 188. 169.  96.]\n",
            " [  0.   0.   0.   3.  18.  77. 177. 255. 255. 255. 255. 255. 255. 255.\n",
            "  255. 255. 255. 255. 255. 255. 255. 178.  79.  20.   3.   0.   0.   0.]\n",
            " [  1.   2.   1.   0.   0.   0.  74. 255. 255. 255. 255. 255. 255. 255.\n",
            "  255. 255. 255. 255. 255. 255. 255.  74.   0.   0.   0.   1.   2.   2.]\n",
            " [  0.   0.   0.   1.   2.   2.  74. 255. 255. 255. 255. 255. 255. 255.\n",
            "  255. 255. 255. 255. 255. 255. 255.  74.   2.   2.   1.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.  74. 255. 255. 255. 255. 255. 255. 255.\n",
            "  255. 255. 255. 255. 255. 255. 255.  74.   0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.  74. 255. 255. 255. 255. 255. 255. 255.\n",
            "  255. 255. 255. 255. 255. 255. 255.  74.   0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.  74. 255. 255. 255. 255. 255. 255. 255.\n",
            "  255. 255. 255. 255. 255. 255. 255.  74.   0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.  74. 255. 255. 255. 255. 255. 255. 255.\n",
            "  255. 255. 255. 255. 255. 255. 255.  74.   0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.  74. 255. 255. 255. 255. 255. 255. 255.\n",
            "  255. 255. 255. 255. 255. 255. 255.  74.   0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.  74. 255. 255. 255. 255. 255. 255. 255.\n",
            "  255. 255. 255. 255. 255. 255. 255.  74.   0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.  74. 255. 255. 255. 255. 255. 255. 255.\n",
            "  255. 255. 255. 255. 255. 255. 255.  74.   0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.  74. 255. 255. 255. 255. 255. 255. 255.\n",
            "  255. 255. 255. 255. 255. 255. 255.  74.   0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.  74. 255. 255. 255. 255. 255. 255. 255.\n",
            "  255. 255. 255. 255. 255. 255. 255.  74.   0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.  74. 255. 255. 255. 255. 255. 255. 255.\n",
            "  255. 255. 255. 255. 255. 255. 255.  74.   0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.  74. 255. 255. 255. 255. 255. 255. 255.\n",
            "  255. 255. 255. 255. 255. 255. 255.  74.   0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.  74. 255. 255. 255. 255. 255. 255. 255.\n",
            "  255. 255. 255. 255. 255. 255. 255.  74.   0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.  74. 255. 255. 255. 255. 255. 255. 255.\n",
            "  255. 255. 255. 255. 255. 255. 255.  74.   0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.  74. 255. 255. 255. 255. 255. 255. 255.\n",
            "  255. 255. 255. 255. 255. 255. 255.  74.   0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.  74. 255. 255. 255. 255. 255. 255. 255.\n",
            "  255. 255. 255. 255. 255. 255. 255.  74.   0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.  74. 255. 255. 255. 255. 255. 255. 255.\n",
            "  255. 255. 255. 255. 255. 255. 255.  74.   0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.  74. 255. 255. 255. 255. 255. 255. 255.\n",
            "  255. 255. 255. 255. 255. 255. 255.  74.   0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.  74. 255. 255. 255. 255. 255. 255. 255.\n",
            "  255. 255. 255. 255. 255. 255. 255.  74.   0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   1.   2.   1.  74. 255. 255. 255. 255. 255. 255. 255.\n",
            "  255. 255. 255. 255. 255. 255. 255.  74.   2.   2.   1.   0.   0.   0.]\n",
            " [  1.   2.   1.   0.   0.   0.  74. 255. 255. 255. 255. 255. 255. 255.\n",
            "  255. 255. 255. 255. 255. 255. 255.  74.   0.   0.   0.   1.   2.   2.]\n",
            " [  0.   0.   0.   6.  30.  97. 193. 255. 255. 255. 255. 255. 255. 255.\n",
            "  255. 255. 255. 255. 255. 255. 255. 185.  87.  24.   4.   0.   0.   0.]\n",
            " [ 79. 167. 191. 214. 240. 254. 255. 255. 255. 255. 255. 255. 255. 255.\n",
            "  255. 255. 255. 255. 255. 255. 255. 255. 253. 236. 209. 188. 169.  96.]\n",
            " [124. 236. 255. 255. 255. 254. 253. 255. 255. 255. 255. 255. 255. 255.\n",
            "  255. 255. 255. 255. 255. 255. 255. 253. 254. 255. 255. 255. 240. 149.]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAApElEQVR4nO2SPQ4BURRGz30epYQRJiR2YBkaiShp7cM2LIRGlGqlNdAwfnrep5nmzYgFiNOe+yW3OCwuksJLEa8g6b70h2270UuqRLjn9ZRle4BKc7yKl+tJqwJgzoBRLEeAOeclzCjikIQHpJIM+ckX/vK3pAfMSik4JPBm+pxJXlatP9/F9W2nqQdsMks73Xqxv3A7nrONHwwTUIgfC5YkPC5vvJpsWenFMIQAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "[[-0.01  0.43  0.5   0.5   0.5   0.5   0.49  0.5   0.5   0.5   0.5   0.5\n",
            "   0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.49  0.5   0.5\n",
            "   0.5   0.5   0.44  0.08]\n",
            " [-0.19  0.15  0.23  0.3   0.41  0.48  0.5   0.5   0.5   0.5   0.5   0.5\n",
            "   0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.48  0.41\n",
            "   0.31  0.24  0.16 -0.12]\n",
            " [-0.5  -0.5  -0.5  -0.49 -0.43 -0.2   0.19  0.5   0.5   0.5   0.5   0.5\n",
            "   0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.2  -0.19 -0.42\n",
            "  -0.49 -0.5  -0.5  -0.5 ]\n",
            " [-0.5  -0.49 -0.5  -0.5  -0.5  -0.5  -0.21  0.5   0.5   0.5   0.5   0.5\n",
            "   0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.5  -0.21 -0.5  -0.5\n",
            "  -0.5  -0.5  -0.49 -0.49]\n",
            " [-0.5  -0.5  -0.5  -0.5  -0.49 -0.49 -0.21  0.5   0.5   0.5   0.5   0.5\n",
            "   0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.5  -0.21 -0.49 -0.49\n",
            "  -0.5  -0.5  -0.5  -0.5 ]\n",
            " [-0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.21  0.5   0.5   0.5   0.5   0.5\n",
            "   0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.5  -0.21 -0.5  -0.5\n",
            "  -0.5  -0.5  -0.5  -0.5 ]\n",
            " [-0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.21  0.5   0.5   0.5   0.5   0.5\n",
            "   0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.5  -0.21 -0.5  -0.5\n",
            "  -0.5  -0.5  -0.5  -0.5 ]\n",
            " [-0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.21  0.5   0.5   0.5   0.5   0.5\n",
            "   0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.5  -0.21 -0.5  -0.5\n",
            "  -0.5  -0.5  -0.5  -0.5 ]\n",
            " [-0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.21  0.5   0.5   0.5   0.5   0.5\n",
            "   0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.5  -0.21 -0.5  -0.5\n",
            "  -0.5  -0.5  -0.5  -0.5 ]\n",
            " [-0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.21  0.5   0.5   0.5   0.5   0.5\n",
            "   0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.5  -0.21 -0.5  -0.5\n",
            "  -0.5  -0.5  -0.5  -0.5 ]\n",
            " [-0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.21  0.5   0.5   0.5   0.5   0.5\n",
            "   0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.5  -0.21 -0.5  -0.5\n",
            "  -0.5  -0.5  -0.5  -0.5 ]\n",
            " [-0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.21  0.5   0.5   0.5   0.5   0.5\n",
            "   0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.5  -0.21 -0.5  -0.5\n",
            "  -0.5  -0.5  -0.5  -0.5 ]\n",
            " [-0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.21  0.5   0.5   0.5   0.5   0.5\n",
            "   0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.5  -0.21 -0.5  -0.5\n",
            "  -0.5  -0.5  -0.5  -0.5 ]\n",
            " [-0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.21  0.5   0.5   0.5   0.5   0.5\n",
            "   0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.5  -0.21 -0.5  -0.5\n",
            "  -0.5  -0.5  -0.5  -0.5 ]\n",
            " [-0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.21  0.5   0.5   0.5   0.5   0.5\n",
            "   0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.5  -0.21 -0.5  -0.5\n",
            "  -0.5  -0.5  -0.5  -0.5 ]\n",
            " [-0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.21  0.5   0.5   0.5   0.5   0.5\n",
            "   0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.5  -0.21 -0.5  -0.5\n",
            "  -0.5  -0.5  -0.5  -0.5 ]\n",
            " [-0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.21  0.5   0.5   0.5   0.5   0.5\n",
            "   0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.5  -0.21 -0.5  -0.5\n",
            "  -0.5  -0.5  -0.5  -0.5 ]\n",
            " [-0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.21  0.5   0.5   0.5   0.5   0.5\n",
            "   0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.5  -0.21 -0.5  -0.5\n",
            "  -0.5  -0.5  -0.5  -0.5 ]\n",
            " [-0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.21  0.5   0.5   0.5   0.5   0.5\n",
            "   0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.5  -0.21 -0.5  -0.5\n",
            "  -0.5  -0.5  -0.5  -0.5 ]\n",
            " [-0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.21  0.5   0.5   0.5   0.5   0.5\n",
            "   0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.5  -0.21 -0.5  -0.5\n",
            "  -0.5  -0.5  -0.5  -0.5 ]\n",
            " [-0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.21  0.5   0.5   0.5   0.5   0.5\n",
            "   0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.5  -0.21 -0.5  -0.5\n",
            "  -0.5  -0.5  -0.5  -0.5 ]\n",
            " [-0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.21  0.5   0.5   0.5   0.5   0.5\n",
            "   0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.5  -0.21 -0.5  -0.5\n",
            "  -0.5  -0.5  -0.5  -0.5 ]\n",
            " [-0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.21  0.5   0.5   0.5   0.5   0.5\n",
            "   0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.5  -0.21 -0.5  -0.5\n",
            "  -0.5  -0.5  -0.5  -0.5 ]\n",
            " [-0.5  -0.5  -0.5  -0.5  -0.49 -0.5  -0.21  0.5   0.5   0.5   0.5   0.5\n",
            "   0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.5  -0.21 -0.49 -0.49\n",
            "  -0.5  -0.5  -0.5  -0.5 ]\n",
            " [-0.5  -0.49 -0.5  -0.5  -0.5  -0.5  -0.21  0.5   0.5   0.5   0.5   0.5\n",
            "   0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.5  -0.21 -0.5  -0.5\n",
            "  -0.5  -0.5  -0.49 -0.49]\n",
            " [-0.5  -0.5  -0.5  -0.48 -0.38 -0.12  0.26  0.5   0.5   0.5   0.5   0.5\n",
            "   0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.23 -0.16 -0.41\n",
            "  -0.48 -0.5  -0.5  -0.5 ]\n",
            " [-0.19  0.15  0.25  0.34  0.44  0.5   0.5   0.5   0.5   0.5   0.5   0.5\n",
            "   0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.49  0.43\n",
            "   0.32  0.24  0.16 -0.12]\n",
            " [-0.01  0.43  0.5   0.5   0.5   0.5   0.49  0.5   0.5   0.5   0.5   0.5\n",
            "   0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.49  0.5   0.5\n",
            "   0.5   0.5   0.44  0.08]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAApElEQVR4nO2SPQ4BURRGz30epYQRJiR2YBkaiShp7cM2LIRGlGqlNdAwfnrep5nmzYgFiNOe+yW3OCwuksJLEa8g6b70h2270UuqRLjn9ZRle4BKc7yKl+tJqwJgzoBRLEeAOeclzCjikIQHpJIM+ckX/vK3pAfMSik4JPBm+pxJXlatP9/F9W2nqQdsMks73Xqxv3A7nrONHwwTUIgfC5YkPC5vvJpsWenFMIQAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "rJQe2th45GIV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "4e50881e-7496-47c3-c453-7a94c5ea7940"
      },
      "source": [
        "# Display first 8 images ສະແດງ 8ໂຕທຳອິດ\n",
        "for file in fn[:8]:\n",
        "    path = 'notMNIST_small/I/' + file\n",
        "##    print (path)\n",
        "    display(Image(path))\n",
        "\n",
        "###   ນາເພີ່ມໃສ່  ###\n",
        "###    import pdb; pdb.set_trace()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAFklEQVR4nGP8z4AbMOGRG5UclRxhkgCD/gE3CChK/QAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAApElEQVR4nO2SPQ4BURRGz30epYQRJiR2YBkaiShp7cM2LIRGlGqlNdAwfnrep5nmzYgFiNOe+yW3OCwuksJLEa8g6b70h2270UuqRLjn9ZRle4BKc7yKl+tJqwJgzoBRLEeAOeclzCjikIQHpJIM+ckX/vK3pAfMSik4JPBm+pxJXlatP9/F9W2nqQdsMks73Xqxv3A7nrONHwwTUIgfC5YkPC5vvJpsWenFMIQAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAlElEQVR4nO2SzQ2CQBBG3yyLZgPZarhgURZjC1RAExZE5EeBGQ8k6iZ68Wh8t8nLvNNHa2amagnb3bpuVBAhQQR07CQW+1Dq4TTnDzXnx7O7jNceAEdl0zM6WYUD8CLgNEuzGQ4wbwawpnJFFbb3T/zlb8nvZ+JjsQul1a99R21yGW89zbDYW5ah8TEARjp5Q8hCvAPxhWx0RVTwJwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAkklEQVR4nLXSQQ4BQRCF4ar3CgsRkUYsR8zCEdzBytrF3M0xDE2YaRfwxkL0sr5UKvnTfmK9XdnnBzH/EcPMSlHo7u4KSyly06cWoQ7HzmdprPDI9WapsGF+tgoBkAL/lK+/bV+hIKnb3plvV4E+MkBu7pHmE4UHVvVC4YX58VIIIELg1whdp3DIAeU3OSM3SeAb1lQjdPy8q7gAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABKUlEQVR4nJ2PPU7EMBBGv3EcOxuQgKWBhoKGkhvQcg9ASFxjEQ1HoqTgAtRoG6BAQESU3cRx7KHIxvwkihDT2Jo33xubzq62rTHG2KaxjXUiVlLGOkm0qm7k60OZ6A2Nn1WVeZ3NSU6U0jrd3D8/jAEA/v36/q2oamurMBsfvTAzM3/M0q4nQQSAQM+u7bg5IjAAhgRza8t8C33hVmMQwVu0Q+Cc8Rty1/K570H4LrlAH3ZJtn1tgHDow7DJDsBgcAPaUM1Ycgx+vWwAUjSW/DekiEaSYuD2N60MV9kuIIg0rIp49VfZHgzaWeloi32QKCGE1Ot7F8cJAMDcXT4WlWN4R6cn0yhSk7Xk29pFsbTeF7dy92BqjSmzZd3Y2jaklUzSSaq1Kp8+AdlnfWM/z6g5AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABW0lEQVR4nFVSu0pEMRA9mURQRMXt3IUtt1FsBLnVIojPTn/B1sbSz/BLxErdFTvFSkEQdAsFEUXULcQXmsxY3DxupkrmZOacMxMAADQ25JclBTM76REAQLAGUoIYSinBvvK31gC9bW47DQBwem9rmOnReLAH4gZibffOWIRCkMbYk5S0LN9TMEQRBKEd1Di5Ip+L4DxceWR0maqgsijSu9NUVCbq75HytVmKiZWYHRHlTV/fl54TuAQOlAfIBClrZhLDCbIgTP55Sie3Ne8/VKIwLlBe9rXkPpdTm8O8q8JoT1yY0HTuUqMtHCjPB8PIw5sCkfL8x0gVZKwkaR1Utg4oTDx7SpbPZk5psB7kWDlG3DKVPEWYneAMJmsLdZGMLECnPJFB6yuu66UGTbEtMzA3xMrLPuorx0HLeJ3pYzGpf2jU2N74y45YqQaLlU7YyiqUSPWvC2PXO/0H3cHfy84DRm4AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAtklEQVR4nO2TMW4CUQxEn78RUEZQUABKk4I9yd5hu9yA64G4BUhIFFSkSEn5Jf6fFNkFFgE9EtONZ2wXHlMpScpZV8hZUlJlg6I//JyVYxkNZJvV/nCMWwCMr51S03fS8oN/q7m7d5krNmLUNz13d+soAYFfdJ5KJCWAUPPLRjj7wg1vIdwrvsVXEO2xaO1jN86OBcAYXXqNHi4g17QdzUUdTRsU/eG0KCetUK9X+59j3PLsHf4AAiZ9xo3Jsr8AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABEElEQVR4nK2RvU7DQBCEZ3ftWA7IiZQCCdEiUVAAD0XPw9Aj8Rp0SCBRoVQUlDRpjBJQUPxzQ+G/s2NRMdXdfDujOy2WJEk6DuX4JFczajy/uE4oqERZ375+75x8VvcQd14mv4EBAFTNzASPGZogvh6gZmYauMr4yCZtb5miIABobawKtCrX9UHrqrTsoNsK4Sc3HqQTPwnZOg+iBymZB1vpiMcBlM4ZS/4Jx/S/UPagjEyNJ9mD9JP7tX4yqldbQWHkTenBoHZuHbRZr1ZwHHYwOEJTKxaY4nLavEMQn0MtsHapJy/Mi1o5nxfVpNyfbvLJ4ixm9xfKz1u6C5NlQJsyX71ncCAIgUBlchglFuAXYfd99ZpX94EAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "XBtLSUio5GIY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "0fd504dd-d59d-48b6-a106-5f1d7aac0cc0"
      },
      "source": [
        "###   ນາເພີ່ມໃສ່   ເພື່ອຊອກຈຳນວນຟ້າຍໃນ ໂຟນເດີ(ໄດເລັກໂຕຣີ) ###\n",
        "# list = os.listdir(fn) # fn is your directory path\n",
        "dir = os.listdir(\"notMNIST_large/D/\")\n",
        "number_files = len(dir)\n",
        "print ('ຈຳນວນຮູບໃນໂຟນເດີ [notMNIST_large/D/]: ',number_files,'ຮູບ')\n",
        "\n",
        "dir = os.listdir(\"notMNIST_large/I/\")\n",
        "number_files = len(dir)\n",
        "print ('ຈຳນວນຮູບໃນໂຟນເດີ [notMNIST_large/I/]: ',number_files,'ຮູບ')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ຈຳນວນຮູບໃນໂຟນເດີ [notMNIST_large/D/]:  52912 ຮູບ\n",
            "ຈຳນວນຮູບໃນໂຟນເດີ [notMNIST_large/I/]:  52912 ຮູບ\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "8r62Uhe-5GIb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "04253bf3-9055-4f3f-b501-00fa14399491"
      },
      "source": [
        "print(test_folders)\n",
        "print(path)\n",
        "display(Image(path))\n",
        "image_file = path\n",
        "\n",
        "\n",
        "# import imageio\n",
        "# im = imageio.imread('astronaut.png')\n",
        "\n",
        "image_data_Na = imageio.imread(image_file).astype(float)\n",
        "\n",
        "print(image_data_Na) ### ເບິ່ງຄ່າແຕ່ລະພິເຊວກ່ອນເປັນ zero mean and standard deviation ~0.5 to make training easier down the road\n",
        "\n",
        "pixel_depth_Na = 255.0  # Number of levels per pixel.\n",
        "image_data_Na = (image_data_Na - pixel_depth_Na / 2) / pixel_depth_Na\n",
        "print(image_data_Na) ### ເບິ່ງຄ່າແຕ່ລະພິເຊວເມື່ອປ່ຽນເປັນ zero mean and standard deviation ~0.5 to make training easier down the road"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['./notMNIST_small/A', './notMNIST_small/B', './notMNIST_small/C', './notMNIST_small/D', './notMNIST_small/E', './notMNIST_small/F', './notMNIST_small/G', './notMNIST_small/H', './notMNIST_small/I', './notMNIST_small/J']\n",
            "notMNIST_small/I/RWxsaW5ndG9uIE1UIEJvbGQudHRm.png\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABEElEQVR4nK2RvU7DQBCEZ3ftWA7IiZQCCdEiUVAAD0XPw9Aj8Rp0SCBRoVQUlDRpjBJQUPxzQ+G/s2NRMdXdfDujOy2WJEk6DuX4JFczajy/uE4oqERZ375+75x8VvcQd14mv4EBAFTNzASPGZogvh6gZmYauMr4yCZtb5miIABobawKtCrX9UHrqrTsoNsK4Sc3HqQTPwnZOg+iBymZB1vpiMcBlM4ZS/4Jx/S/UPagjEyNJ9mD9JP7tX4yqldbQWHkTenBoHZuHbRZr1ZwHHYwOEJTKxaY4nLavEMQn0MtsHapJy/Mi1o5nxfVpNyfbvLJ4ixm9xfKz1u6C5NlQJsyX71ncCAIgUBlchglFuAXYfd99ZpX94EAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "[[210. 255. 255. 255. 255. 255. 255. 254. 255. 255. 255. 255. 255. 255.\n",
            "  255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255. 254. 255. 198.]\n",
            " [ 50.  65.  64.  66.  75.  91. 139. 241. 255. 254. 255. 255. 255. 255.\n",
            "  255. 255. 255. 255. 254. 255. 240. 133.  85.  73.  66.  64.  65.  48.]\n",
            " [  0.   0.   0.   0.   0.   5.   0. 160. 255. 255. 255. 255. 255. 255.\n",
            "  255. 255. 255. 255. 255. 251. 113.   0.   3.   0.   0.   0.   0.   0.]\n",
            " [  2.   3.   3.   3.   3.   6.   0.  99. 249. 255. 255. 255. 255. 255.\n",
            "  255. 255. 255. 254. 255. 238.  45.   0.   5.   3.   3.   3.   3.   2.]\n",
            " [  0.   0.   0.   0.   0.   2.   0.  71. 243. 255. 254. 255. 255. 255.\n",
            "  255. 255. 255. 254. 255. 235.  27.   0.   1.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   2.   0.  48. 239. 255. 254. 255. 255. 255.\n",
            "  255. 255. 255. 254. 255. 232.  12.   0.   1.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   1.   0.  30. 236. 255. 254. 255. 255. 255.\n",
            "  255. 255. 255. 254. 255. 230.   2.   1.   0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   1.   0.  16. 233. 255. 254. 255. 255. 255.\n",
            "  255. 255. 255. 254. 255. 229.   0.   2.   0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   1.   1.   6. 231. 255. 254. 255. 255. 255.\n",
            "  255. 255. 255. 254. 255. 228.   0.   3.   0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.   2.   0. 229. 255. 254. 255. 255. 255.\n",
            "  255. 255. 255. 254. 255. 228.   0.   3.   0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.   2.   0. 229. 255. 254. 255. 255. 255.\n",
            "  255. 255. 255. 254. 255. 227.   0.   3.   0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.   3.   0. 228. 255. 254. 255. 255. 255.\n",
            "  255. 255. 255. 254. 255. 227.   0.   3.   0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.   3.   0. 227. 255. 254. 255. 255. 255.\n",
            "  255. 255. 255. 254. 255. 227.   0.   3.   0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.   3.   0. 227. 255. 254. 255. 255. 255.\n",
            "  255. 255. 255. 254. 255. 227.   0.   3.   0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.   3.   0. 227. 255. 254. 255. 255. 255.\n",
            "  255. 255. 255. 254. 255. 227.   0.   3.   0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.   3.   0. 227. 255. 254. 255. 255. 255.\n",
            "  255. 255. 255. 254. 255. 227.   0.   3.   0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.   3.   0. 227. 255. 254. 255. 255. 255.\n",
            "  255. 255. 255. 254. 255. 228.   0.   3.   0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.   3.   0. 228. 255. 254. 255. 255. 255.\n",
            "  255. 255. 255. 254. 255. 228.   0.   3.   0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.   3.   0. 228. 255. 254. 255. 255. 255.\n",
            "  255. 255. 255. 254. 255. 229.   0.   2.   0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.   2.   0. 229. 255. 254. 255. 255. 255.\n",
            "  255. 255. 255. 254. 255. 230.   0.   2.   0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.   2.   0. 229. 255. 254. 255. 255. 255.\n",
            "  255. 255. 255. 254. 255. 231.   7.   1.   1.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   1.   1.   7. 231. 255. 254. 255. 255. 255.\n",
            "  255. 255. 255. 254. 255. 233.  18.   0.   1.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   1.   0.  23. 234. 255. 254. 255. 255. 255.\n",
            "  255. 255. 255. 254. 255. 236.  33.   0.   1.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   2.   0.  49. 239. 255. 254. 255. 255. 255.\n",
            "  255. 255. 255. 254. 255. 240.  56.   0.   2.   0.   0.   0.   0.   0.]\n",
            " [  0.   1.   3.   4.   3.   4.   0.  98. 249. 255. 255. 255. 255. 255.\n",
            "  255. 255. 255. 255. 255. 249. 100.   0.   4.   3.   4.   3.   1.   0.]\n",
            " [  0.   0.   0.   0.   2.   0.  28. 202. 255. 251. 252. 252. 252. 252.\n",
            "  252. 252. 252. 252. 251. 255. 199.  19.   0.   1.   0.   0.   0.   0.]\n",
            " [163. 200. 186. 181. 187. 206. 246. 255. 254. 255. 255. 255. 255. 255.\n",
            "  255. 255. 255. 255. 255. 254. 255. 246. 206. 188. 181. 186. 200. 154.]\n",
            " [162. 203. 196. 195. 190. 183. 170. 164. 164. 162. 162. 161. 161. 160.\n",
            "  160. 161. 161. 162. 162. 164. 165. 171. 183. 190. 195. 198. 204. 154.]]\n",
            "[[ 0.32  0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.5\n",
            "   0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.5\n",
            "   0.5   0.5   0.5   0.28]\n",
            " [-0.3  -0.25 -0.25 -0.24 -0.21 -0.14  0.05  0.45  0.5   0.5   0.5   0.5\n",
            "   0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.44  0.02 -0.17 -0.21\n",
            "  -0.24 -0.25 -0.25 -0.31]\n",
            " [-0.5  -0.5  -0.5  -0.5  -0.5  -0.48 -0.5   0.13  0.5   0.5   0.5   0.5\n",
            "   0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.48 -0.06 -0.5  -0.49 -0.5\n",
            "  -0.5  -0.5  -0.5  -0.5 ]\n",
            " [-0.49 -0.49 -0.49 -0.49 -0.49 -0.48 -0.5  -0.11  0.48  0.5   0.5   0.5\n",
            "   0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.43 -0.32 -0.5  -0.48 -0.49\n",
            "  -0.49 -0.49 -0.49 -0.49]\n",
            " [-0.5  -0.5  -0.5  -0.5  -0.5  -0.49 -0.5  -0.22  0.45  0.5   0.5   0.5\n",
            "   0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.42 -0.39 -0.5  -0.5  -0.5\n",
            "  -0.5  -0.5  -0.5  -0.5 ]\n",
            " [-0.5  -0.5  -0.5  -0.5  -0.5  -0.49 -0.5  -0.31  0.44  0.5   0.5   0.5\n",
            "   0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.41 -0.45 -0.5  -0.5  -0.5\n",
            "  -0.5  -0.5  -0.5  -0.5 ]\n",
            " [-0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.38  0.43  0.5   0.5   0.5\n",
            "   0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.4  -0.49 -0.5  -0.5  -0.5\n",
            "  -0.5  -0.5  -0.5  -0.5 ]\n",
            " [-0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.44  0.41  0.5   0.5   0.5\n",
            "   0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.4  -0.5  -0.49 -0.5  -0.5\n",
            "  -0.5  -0.5  -0.5  -0.5 ]\n",
            " [-0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.48  0.41  0.5   0.5   0.5\n",
            "   0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.39 -0.5  -0.49 -0.5  -0.5\n",
            "  -0.5  -0.5  -0.5  -0.5 ]\n",
            " [-0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.49 -0.5   0.4   0.5   0.5   0.5\n",
            "   0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.39 -0.5  -0.49 -0.5  -0.5\n",
            "  -0.5  -0.5  -0.5  -0.5 ]\n",
            " [-0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.49 -0.5   0.4   0.5   0.5   0.5\n",
            "   0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.39 -0.5  -0.49 -0.5  -0.5\n",
            "  -0.5  -0.5  -0.5  -0.5 ]\n",
            " [-0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.49 -0.5   0.39  0.5   0.5   0.5\n",
            "   0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.39 -0.5  -0.49 -0.5  -0.5\n",
            "  -0.5  -0.5  -0.5  -0.5 ]\n",
            " [-0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.49 -0.5   0.39  0.5   0.5   0.5\n",
            "   0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.39 -0.5  -0.49 -0.5  -0.5\n",
            "  -0.5  -0.5  -0.5  -0.5 ]\n",
            " [-0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.49 -0.5   0.39  0.5   0.5   0.5\n",
            "   0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.39 -0.5  -0.49 -0.5  -0.5\n",
            "  -0.5  -0.5  -0.5  -0.5 ]\n",
            " [-0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.49 -0.5   0.39  0.5   0.5   0.5\n",
            "   0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.39 -0.5  -0.49 -0.5  -0.5\n",
            "  -0.5  -0.5  -0.5  -0.5 ]\n",
            " [-0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.49 -0.5   0.39  0.5   0.5   0.5\n",
            "   0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.39 -0.5  -0.49 -0.5  -0.5\n",
            "  -0.5  -0.5  -0.5  -0.5 ]\n",
            " [-0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.49 -0.5   0.39  0.5   0.5   0.5\n",
            "   0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.39 -0.5  -0.49 -0.5  -0.5\n",
            "  -0.5  -0.5  -0.5  -0.5 ]\n",
            " [-0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.49 -0.5   0.39  0.5   0.5   0.5\n",
            "   0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.39 -0.5  -0.49 -0.5  -0.5\n",
            "  -0.5  -0.5  -0.5  -0.5 ]\n",
            " [-0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.49 -0.5   0.39  0.5   0.5   0.5\n",
            "   0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.4  -0.5  -0.49 -0.5  -0.5\n",
            "  -0.5  -0.5  -0.5  -0.5 ]\n",
            " [-0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.49 -0.5   0.4   0.5   0.5   0.5\n",
            "   0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.4  -0.5  -0.49 -0.5  -0.5\n",
            "  -0.5  -0.5  -0.5  -0.5 ]\n",
            " [-0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.49 -0.5   0.4   0.5   0.5   0.5\n",
            "   0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.41 -0.47 -0.5  -0.5  -0.5\n",
            "  -0.5  -0.5  -0.5  -0.5 ]\n",
            " [-0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.47  0.41  0.5   0.5   0.5\n",
            "   0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.41 -0.43 -0.5  -0.5  -0.5\n",
            "  -0.5  -0.5  -0.5  -0.5 ]\n",
            " [-0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.41  0.42  0.5   0.5   0.5\n",
            "   0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.43 -0.37 -0.5  -0.5  -0.5\n",
            "  -0.5  -0.5  -0.5  -0.5 ]\n",
            " [-0.5  -0.5  -0.5  -0.5  -0.5  -0.49 -0.5  -0.31  0.44  0.5   0.5   0.5\n",
            "   0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.44 -0.28 -0.5  -0.49 -0.5\n",
            "  -0.5  -0.5  -0.5  -0.5 ]\n",
            " [-0.5  -0.5  -0.49 -0.48 -0.49 -0.48 -0.5  -0.12  0.48  0.5   0.5   0.5\n",
            "   0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.48 -0.11 -0.5  -0.48 -0.49\n",
            "  -0.48 -0.49 -0.5  -0.5 ]\n",
            " [-0.5  -0.5  -0.5  -0.5  -0.49 -0.5  -0.39  0.29  0.5   0.48  0.49  0.49\n",
            "   0.49  0.49  0.49  0.49  0.49  0.49  0.48  0.5   0.28 -0.43 -0.5  -0.5\n",
            "  -0.5  -0.5  -0.5  -0.5 ]\n",
            " [ 0.14  0.28  0.23  0.21  0.23  0.31  0.46  0.5   0.5   0.5   0.5   0.5\n",
            "   0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.46  0.31  0.24\n",
            "   0.21  0.23  0.28  0.1 ]\n",
            " [ 0.14  0.3   0.27  0.26  0.25  0.22  0.17  0.14  0.14  0.14  0.14  0.13\n",
            "   0.13  0.13  0.13  0.13  0.13  0.14  0.14  0.14  0.15  0.17  0.22  0.25\n",
            "   0.26  0.28  0.3   0.1 ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "deletable": true,
        "editable": true,
        "id": "PBdkjESPK8tw"
      },
      "source": [
        "Now let's load the data in a more manageable format. Since, depending on your computer setup you might not be able to fit it all in memory, we'll load each class into a separate dataset, store them on disk and curate them independently. Later we'll merge them into a single dataset of manageable size.\n",
        "\n",
        "We'll convert the entire dataset into a 3D array (image index, x, y) of floating point values, normalized to have approximately zero mean and standard deviation ~0.5 to make training easier down the road.\n",
        " \n",
        "ພວກເຮົາຈະແປງຊຸດຂໍ້ມູນເປັນ 3D array (image index, x, y) ເປັນຕົວເລກປະເພດ float ໃນຮູບຕົວເລກ zero mean ແລະ standard deviation ~0.5 ເພື່ອງ້າຍໃນການປະຕິບັດ\n",
        "\n",
        "A few images might not be readable, we'll just skip them.\n",
        "\n",
        "..... ໝາຍເຫດ .....: os.listdir(folder) ໝາຍເຖິງຊື່ file ທັງໝົດທີ່ມີຢູ່ໃນໂຟເດີ ຕາມທີ່ຢູ່ທີ່ບອກດ້ວຍຕົວປ່ຽນ \"folder\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "colab_type": "code",
        "deletable": true,
        "editable": true,
        "id": "h7q0XhG3MJdf",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "1657a0d0-fed7-4a7c-c0cb-385fb581e1bc"
      },
      "source": [
        "image_size = 28  # Pixel width and height.\n",
        "pixel_depth = 255.0  # Number of levels per pixel.\n",
        "\n",
        "def load_letter(folder, min_num_images):\n",
        "  \"\"\"Load the data for a single letter label.\"\"\"\n",
        "\n",
        "###  print (\" ຢູ່ໃນ load_letter method\")\n",
        "\n",
        "  image_files = os.listdir(folder)                              # ໂລດຊື່ file ທັງໝົດທີ່ມີຢູ່ໃນໂຟເດີ (folder) ໃສ່ image_files\n",
        "  dataset = np.ndarray(shape=(len(image_files), image_size, image_size),\n",
        "                         dtype=np.float32)                      # np.ndarray ແມ່ນປ່ຽນຮູບທັງໝົດເປັນຕົວປ່ຽນ tuple ໂຕດ່ຽວ\n",
        "  print('ຂະໜາດ dataset ກ່ອນຂັດຮູບໃຊ້ບໍ່ໄດ້ອອກ: ', dataset.shape)     ###   ແຖວນີ້ນາເພີ່ມໃສ່\n",
        "  print(folder)\n",
        "  \n",
        "  num_images = 0\n",
        "  for image in image_files:\n",
        "    image_file = os.path.join(folder, image)\n",
        "    try:                                                         # ຂັ້ນຕອນນີ້ພະຍາມອ່ານຟ້າຍຮູບຖ້າອ່ານບໍ່ໄດ້ກໍ່ຈະຂັດອອກ\n",
        "\n",
        "###      image_data = (ndimage.imread(image_file).astype(float) -   # ຄຳສັງເກົ່າໃຊ້ເກັບ Pyton V2.7 \"ndimage\"\n",
        "      image_data = (imageio.imread(image_file).astype(float) -   # ຂັ້ນຕອນນີ້ຄິດວ່າແມ່ນ່ປ່ຽນຮູບຕົວເລກແຕ່ລະພິກເຊວຮູບເປັນຮູບແບບ zero mean and standard deviation ~0.5     \n",
        "                    pixel_depth / 2) / pixel_depth               # to make training easier down the road ເພື່ອງ້າຍໃນການປະຜົນຂອງຄອມ\n",
        "\n",
        "\n",
        "      if image_data.shape != (image_size, image_size):\n",
        "        raise Exception('Unexpected image shape: %s' % str(image_data.shape))\n",
        "      dataset[num_images, :, :] = image_data\n",
        "      num_images = num_images + 1\n",
        "###    except IOError as e:        ### # ຄຳສັງເກົ່າໃຊ້ເກັບ Pyton V2.7 \"IOError\"\n",
        "    except Exception as e:\n",
        "\n",
        "      print('Could not read:', image_file, ':', e, '- it\\'s ok, skipping.')\n",
        "    \n",
        "      print (\"image ທີ່: \" + str(num_images) + \"ສະແດງຄ່າ image_file: \" + image_file)   \n",
        "\n",
        "  dataset = dataset[0:num_images, :, :]     ### ແຖວນີ້ອັບເດດຈຳນວນຮູບຫຼັງຈາກຄັດຮູບທີ່ໃຊ້ບໍ່ໄດ້ອອກແລະຈັດລຽນຮູບເຂົ້າມາພ້ອມແລ້ວ\n",
        "  \n",
        "                                         ### ເພື່ອຂະໜາດຂອງອາເລເທົ່າຈຳນວນຮູບທີ່ໃຊ້ໄດ້\n",
        "###   171224 ນາ    ####\n",
        "\n",
        "  if num_images < min_num_images:        ### ຫຼັງຈາກຄັດເອົາຮູບທີ່ໃຊ້ບໍ່ໄດ້ເອົາແລ້ວ ກໍ່ກວດເບິ່ງຈຳນວນທີ່ໃຊ້ໄດ້ມັນໃຫຍ່ກວ່າຈຳນວນຕໍ່າສຸດທີ່ເຮົາຕ້ອງການຫລືບໍ່\n",
        "    raise Exception('Many fewer images than expected: %d < %d' %\n",
        "                    (num_images, min_num_images))\n",
        "    \n",
        "  print('Full dataset tensor:', dataset.shape)\n",
        "  print('Mean:', np.mean(dataset))           ### ???\n",
        "  print('Standard deviation:', np.std(dataset))   ### ???\n",
        "  return dataset\n",
        "\n",
        "###   171225 ນາ    ####\n",
        "\n",
        "def maybe_pickle(data_folders, min_num_images_per_class, force=False):\n",
        "\n",
        "  print (data_folders)\n",
        "  dataset_names = []\n",
        "  for folder in data_folders:                               # ຮອບທຳອິດ ເອົາຄ່າຫ້ອງທີ່1 ຂອງອາເລ້ data_folders ໃສ່ folder ຮອບຕໍ່ໄປກໍ່ເອົາຫ້ອງຕໍ່ໄປຈົນຮອດສຸດທ້າຍ\n",
        "    set_filename = folder + '.pickle'                       # ເຊັນ './notMNIST_large/A', './notMNIST_large/B', ...\n",
        "    dataset_names.append(set_filename)                      # ລວມທີ່ຢູ່ພ້ອມຊື່ຟ້າຍນາມສະກຸນ .pickle ຂອງແຕ່ລະຕົວອັກສອນໃສ່ dataset_names ເພື່ອ\n",
        "    \n",
        "    print (set_filename)\n",
        "\n",
        "    if os.path.exists(set_filename) and force:          # ຫຼັງເປັນຮູບເປັນຮູບເປັນຟ້າຍ .pickle ກໍ່ return ພຽງທີ່ຢູ່ພ້ອມຊື່\n",
        "      # You may override by setting force=True.\n",
        "      print('%s already present - Skipping pickling.' % set_filename)\n",
        "    else:\n",
        "      print('Pickling %s.' % set_filename)\n",
        "      dataset = load_letter(folder, min_num_images_per_class) # ແຖວນີ້ໂລດກຸ່ມຮູບໃສ່ອາເລໂຕດຽວເພື່ອໄວ້ປ່ຽນເປັນຟ້າຍ .pickle\n",
        "      try:\n",
        "        with open(set_filename, 'wb') as f:                  #  ເປີດຟາຍຕາມ set_filename ໃນແບບ 'wb' ເປັນໄບນາລີແລະສາມາດຂຽນໃສ່ຟາຍໄດ້\n",
        "          pickle.dump(dataset, f, pickle.HIGHEST_PROTOCOL)   # ແຖວນີ້ປ່ຽນອາເລເປັນຟ້າຍ .pickle  ຕົວຢ່າງ A.pickle\n",
        "      except Exception as e:                                 # ຕົວຢ່າງຂອງ pickle.dump ຢູ່ https://kite.com/docs/python;pickle.dump\n",
        "        print('Unable to save data to', set_filename, ':', e)       \n",
        "  print(\"ຄ່າ dataset_names ກ່ອນສົ່ງກັບ\" + str(dataset_names))\n",
        "  return dataset_names                                       # ສົ່ງຄືນຊື່ ແລະ ທີ່ຢູ່ຂອງ dataset ໂດຍເປັນ list ຂອງstring ເຊັນ ['./notMNIST_large/A.pickle', './notMNIST_large/B.pickle', './notMNIST_large/C.pickle', './notMNIST_large/D.pickle',\n",
        "                                                             # './notMNIST_large/E.pickle', './notMNIST_large/F.pickle', './notMNIST_large/G.pickle', './notMNIST_large/H.pickle', './notMNIST_large/I.pickle', './notMNIST_large/J.pickle']\n",
        "###print (type(train_folders))\n",
        "\n",
        "print (\"ກ່ອນເລີ້ມ pickle: \" + str(train_folders[0]))\n",
        "train_datasets = maybe_pickle(train_folders, 45000)\n",
        "test_datasets = maybe_pickle(test_folders, 1800)\n",
        "\n",
        "###   200421 ນາ    ####\n",
        "###   171229 ນາ    ####"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ກ່ອນເລີ້ມ pickle: ./notMNIST_large/A\n",
            "['./notMNIST_large/A', './notMNIST_large/B', './notMNIST_large/C', './notMNIST_large/D', './notMNIST_large/E', './notMNIST_large/F', './notMNIST_large/G', './notMNIST_large/H', './notMNIST_large/I', './notMNIST_large/J']\n",
            "./notMNIST_large/A.pickle\n",
            "Pickling ./notMNIST_large/A.pickle.\n",
            "ຂະໜາດ dataset ກ່ອນຂັດຮູບໃຊ້ບໍ່ໄດ້ອອກ:  (52912, 28, 28)\n",
            "./notMNIST_large/A\n",
            "Could not read: ./notMNIST_large/A/SG90IE11c3RhcmQgQlROIFBvc3Rlci50dGY=.png : Could not find a format to read the specified file in mode 'i' - it's ok, skipping.\n",
            "image ທີ່: 16403ສະແດງຄ່າ image_file: ./notMNIST_large/A/SG90IE11c3RhcmQgQlROIFBvc3Rlci50dGY=.png\n",
            "Could not read: ./notMNIST_large/A/Um9tYW5hIEJvbGQucGZi.png : Could not find a format to read the specified file in mode 'i' - it's ok, skipping.\n",
            "image ທີ່: 31272ສະແດງຄ່າ image_file: ./notMNIST_large/A/Um9tYW5hIEJvbGQucGZi.png\n",
            "Could not read: ./notMNIST_large/A/RnJlaWdodERpc3BCb29rSXRhbGljLnR0Zg==.png : Could not find a format to read the specified file in mode 'i' - it's ok, skipping.\n",
            "image ທີ່: 35336ສະແດງຄ່າ image_file: ./notMNIST_large/A/RnJlaWdodERpc3BCb29rSXRhbGljLnR0Zg==.png\n",
            "Full dataset tensor: (52909, 28, 28)\n",
            "Mean: -0.1282502\n",
            "Standard deviation: 0.44312054\n",
            "./notMNIST_large/B.pickle\n",
            "Pickling ./notMNIST_large/B.pickle.\n",
            "ຂະໜາດ dataset ກ່ອນຂັດຮູບໃຊ້ບໍ່ໄດ້ອອກ:  (52912, 28, 28)\n",
            "./notMNIST_large/B\n",
            "Could not read: ./notMNIST_large/B/TmlraXNFRi1TZW1pQm9sZEl0YWxpYy5vdGY=.png : Could not find a format to read the specified file in mode 'i' - it's ok, skipping.\n",
            "image ທີ່: 24429ສະແດງຄ່າ image_file: ./notMNIST_large/B/TmlraXNFRi1TZW1pQm9sZEl0YWxpYy5vdGY=.png\n",
            "Full dataset tensor: (52911, 28, 28)\n",
            "Mean: -0.0075630406\n",
            "Standard deviation: 0.45449123\n",
            "./notMNIST_large/C.pickle\n",
            "Pickling ./notMNIST_large/C.pickle.\n",
            "ຂະໜາດ dataset ກ່ອນຂັດຮູບໃຊ້ບໍ່ໄດ້ອອກ:  (52912, 28, 28)\n",
            "./notMNIST_large/C\n",
            "Full dataset tensor: (52912, 28, 28)\n",
            "Mean: -0.14225793\n",
            "Standard deviation: 0.43980625\n",
            "./notMNIST_large/D.pickle\n",
            "Pickling ./notMNIST_large/D.pickle.\n",
            "ຂະໜາດ dataset ກ່ອນຂັດຮູບໃຊ້ບໍ່ໄດ້ອອກ:  (52912, 28, 28)\n",
            "./notMNIST_large/D\n",
            "Could not read: ./notMNIST_large/D/VHJhbnNpdCBCb2xkLnR0Zg==.png : Could not find a format to read the specified file in mode 'i' - it's ok, skipping.\n",
            "image ທີ່: 51317ສະແດງຄ່າ image_file: ./notMNIST_large/D/VHJhbnNpdCBCb2xkLnR0Zg==.png\n",
            "Full dataset tensor: (52911, 28, 28)\n",
            "Mean: -0.057367794\n",
            "Standard deviation: 0.45564747\n",
            "./notMNIST_large/E.pickle\n",
            "Pickling ./notMNIST_large/E.pickle.\n",
            "ຂະໜາດ dataset ກ່ອນຂັດຮູບໃຊ້ບໍ່ໄດ້ອອກ:  (52912, 28, 28)\n",
            "./notMNIST_large/E\n",
            "Full dataset tensor: (52912, 28, 28)\n",
            "Mean: -0.069899075\n",
            "Standard deviation: 0.45294136\n",
            "./notMNIST_large/F.pickle\n",
            "Pickling ./notMNIST_large/F.pickle.\n",
            "ຂະໜາດ dataset ກ່ອນຂັດຮູບໃຊ້ບໍ່ໄດ້ອອກ:  (52912, 28, 28)\n",
            "./notMNIST_large/F\n",
            "Full dataset tensor: (52912, 28, 28)\n",
            "Mean: -0.1255832\n",
            "Standard deviation: 0.44709012\n",
            "./notMNIST_large/G.pickle\n",
            "Pickling ./notMNIST_large/G.pickle.\n",
            "ຂະໜາດ dataset ກ່ອນຂັດຮູບໃຊ້ບໍ່ໄດ້ອອກ:  (52912, 28, 28)\n",
            "./notMNIST_large/G\n",
            "Full dataset tensor: (52912, 28, 28)\n",
            "Mean: -0.09458155\n",
            "Standard deviation: 0.44623995\n",
            "./notMNIST_large/H.pickle\n",
            "Pickling ./notMNIST_large/H.pickle.\n",
            "ຂະໜາດ dataset ກ່ອນຂັດຮູບໃຊ້ບໍ່ໄດ້ອອກ:  (52912, 28, 28)\n",
            "./notMNIST_large/H\n",
            "Full dataset tensor: (52912, 28, 28)\n",
            "Mean: -0.06852215\n",
            "Standard deviation: 0.45423153\n",
            "./notMNIST_large/I.pickle\n",
            "Pickling ./notMNIST_large/I.pickle.\n",
            "ຂະໜາດ dataset ກ່ອນຂັດຮູບໃຊ້ບໍ່ໄດ້ອອກ:  (52912, 28, 28)\n",
            "./notMNIST_large/I\n",
            "Full dataset tensor: (52912, 28, 28)\n",
            "Mean: 0.030786302\n",
            "Standard deviation: 0.46889862\n",
            "./notMNIST_large/J.pickle\n",
            "Pickling ./notMNIST_large/J.pickle.\n",
            "ຂະໜາດ dataset ກ່ອນຂັດຮູບໃຊ້ບໍ່ໄດ້ອອກ:  (52911, 28, 28)\n",
            "./notMNIST_large/J\n",
            "Full dataset tensor: (52911, 28, 28)\n",
            "Mean: -0.15335818\n",
            "Standard deviation: 0.4436566\n",
            "ຄ່າ dataset_names ກ່ອນສົ່ງກັບ['./notMNIST_large/A.pickle', './notMNIST_large/B.pickle', './notMNIST_large/C.pickle', './notMNIST_large/D.pickle', './notMNIST_large/E.pickle', './notMNIST_large/F.pickle', './notMNIST_large/G.pickle', './notMNIST_large/H.pickle', './notMNIST_large/I.pickle', './notMNIST_large/J.pickle']\n",
            "['./notMNIST_small/A', './notMNIST_small/B', './notMNIST_small/C', './notMNIST_small/D', './notMNIST_small/E', './notMNIST_small/F', './notMNIST_small/G', './notMNIST_small/H', './notMNIST_small/I', './notMNIST_small/J']\n",
            "./notMNIST_small/A.pickle\n",
            "Pickling ./notMNIST_small/A.pickle.\n",
            "ຂະໜາດ dataset ກ່ອນຂັດຮູບໃຊ້ບໍ່ໄດ້ອອກ:  (1873, 28, 28)\n",
            "./notMNIST_small/A\n",
            "Could not read: ./notMNIST_small/A/RGVtb2NyYXRpY2FCb2xkT2xkc3R5bGUgQm9sZC50dGY=.png : Could not find a format to read the specified file in mode 'i' - it's ok, skipping.\n",
            "image ທີ່: 132ສະແດງຄ່າ image_file: ./notMNIST_small/A/RGVtb2NyYXRpY2FCb2xkT2xkc3R5bGUgQm9sZC50dGY=.png\n",
            "Full dataset tensor: (1872, 28, 28)\n",
            "Mean: -0.1326264\n",
            "Standard deviation: 0.44512793\n",
            "./notMNIST_small/B.pickle\n",
            "Pickling ./notMNIST_small/B.pickle.\n",
            "ຂະໜາດ dataset ກ່ອນຂັດຮູບໃຊ້ບໍ່ໄດ້ອອກ:  (1873, 28, 28)\n",
            "./notMNIST_small/B\n",
            "Full dataset tensor: (1873, 28, 28)\n",
            "Mean: 0.0053560827\n",
            "Standard deviation: 0.45711538\n",
            "./notMNIST_small/C.pickle\n",
            "Pickling ./notMNIST_small/C.pickle.\n",
            "ຂະໜາດ dataset ກ່ອນຂັດຮູບໃຊ້ບໍ່ໄດ້ອອກ:  (1873, 28, 28)\n",
            "./notMNIST_small/C\n",
            "Full dataset tensor: (1873, 28, 28)\n",
            "Mean: -0.14152056\n",
            "Standard deviation: 0.4426902\n",
            "./notMNIST_small/D.pickle\n",
            "Pickling ./notMNIST_small/D.pickle.\n",
            "ຂະໜາດ dataset ກ່ອນຂັດຮູບໃຊ້ບໍ່ໄດ້ອອກ:  (1873, 28, 28)\n",
            "./notMNIST_small/D\n",
            "Full dataset tensor: (1873, 28, 28)\n",
            "Mean: -0.049216673\n",
            "Standard deviation: 0.4597589\n",
            "./notMNIST_small/E.pickle\n",
            "Pickling ./notMNIST_small/E.pickle.\n",
            "ຂະໜາດ dataset ກ່ອນຂັດຮູບໃຊ້ບໍ່ໄດ້ອອກ:  (1873, 28, 28)\n",
            "./notMNIST_small/E\n",
            "Full dataset tensor: (1873, 28, 28)\n",
            "Mean: -0.059914764\n",
            "Standard deviation: 0.45734963\n",
            "./notMNIST_small/F.pickle\n",
            "Pickling ./notMNIST_small/F.pickle.\n",
            "ຂະໜາດ dataset ກ່ອນຂັດຮູບໃຊ້ບໍ່ໄດ້ອອກ:  (1873, 28, 28)\n",
            "./notMNIST_small/F\n",
            "Could not read: ./notMNIST_small/F/Q3Jvc3NvdmVyIEJvbGRPYmxpcXVlLnR0Zg==.png : Could not find a format to read the specified file in mode 'i' - it's ok, skipping.\n",
            "image ທີ່: 1218ສະແດງຄ່າ image_file: ./notMNIST_small/F/Q3Jvc3NvdmVyIEJvbGRPYmxpcXVlLnR0Zg==.png\n",
            "Full dataset tensor: (1872, 28, 28)\n",
            "Mean: -0.11818528\n",
            "Standard deviation: 0.45227864\n",
            "./notMNIST_small/G.pickle\n",
            "Pickling ./notMNIST_small/G.pickle.\n",
            "ຂະໜາດ dataset ກ່ອນຂັດຮູບໃຊ້ບໍ່ໄດ້ອອກ:  (1872, 28, 28)\n",
            "./notMNIST_small/G\n",
            "Full dataset tensor: (1872, 28, 28)\n",
            "Mean: -0.09255034\n",
            "Standard deviation: 0.44900584\n",
            "./notMNIST_small/H.pickle\n",
            "Pickling ./notMNIST_small/H.pickle.\n",
            "ຂະໜາດ dataset ກ່ອນຂັດຮູບໃຊ້ບໍ່ໄດ້ອອກ:  (1872, 28, 28)\n",
            "./notMNIST_small/H\n",
            "Full dataset tensor: (1872, 28, 28)\n",
            "Mean: -0.05868925\n",
            "Standard deviation: 0.45875898\n",
            "./notMNIST_small/I.pickle\n",
            "Pickling ./notMNIST_small/I.pickle.\n",
            "ຂະໜາດ dataset ກ່ອນຂັດຮູບໃຊ້ບໍ່ໄດ້ອອກ:  (1872, 28, 28)\n",
            "./notMNIST_small/I\n",
            "Full dataset tensor: (1872, 28, 28)\n",
            "Mean: 0.05264509\n",
            "Standard deviation: 0.47189346\n",
            "./notMNIST_small/J.pickle\n",
            "Pickling ./notMNIST_small/J.pickle.\n",
            "ຂະໜາດ dataset ກ່ອນຂັດຮູບໃຊ້ບໍ່ໄດ້ອອກ:  (1872, 28, 28)\n",
            "./notMNIST_small/J\n",
            "Full dataset tensor: (1872, 28, 28)\n",
            "Mean: -0.1516892\n",
            "Standard deviation: 0.44801363\n",
            "ຄ່າ dataset_names ກ່ອນສົ່ງກັບ['./notMNIST_small/A.pickle', './notMNIST_small/B.pickle', './notMNIST_small/C.pickle', './notMNIST_small/D.pickle', './notMNIST_small/E.pickle', './notMNIST_small/F.pickle', './notMNIST_small/G.pickle', './notMNIST_small/H.pickle', './notMNIST_small/I.pickle', './notMNIST_small/J.pickle']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "deletable": true,
        "editable": true,
        "id": "vUdbskYE2d87"
      },
      "source": [
        "---\n",
        "Problem 2\n",
        "---------\n",
        "\n",
        "Let's verify that the data still looks good. Displaying a sample of the labels and images from the ndarray. Hint: you can use matplotlib.pyplot.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "j9_wU-gJ5GIk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "80d558c1-ac70-4ec9-c5e8-6e43f7056315"
      },
      "source": [
        "\n",
        "# index 0 should be all As, 1 = all Bs, etc. ອິນເດັດ(Index ຫຼື Label) 0 ໝາຍເຖິງກຸ່ມຕົວອັກສອນ 'A', ອິນເດັດ 1 ໝາຍເຖິງ  'B'\n",
        "\n",
        "pickle_file = train_datasets[8]  \n",
        "\n",
        "# With would automatically close the file after the nested block of code\n",
        "with open(pickle_file, 'rb') as f:\n",
        "    \n",
        "    # unpickle\n",
        "    letter_set = pickle.load(f)  \n",
        "    \n",
        "    # pick a random image index\n",
        "    ### sample_idx = 0      # The method len() returns the number of elements in the list ເລືອກເອົາຮູບທຳອິດຂອງ ໃນກຸ່ມຕົວອັກສອນທີ່ເລືອກ\n",
        "    sample_idx = np.random.randint(len(letter_set))      # The method len() returns the number of elements in the list\n",
        "    print(\"ຄວາມກວ້າງຂອງ len(letter_set): \" + str(len(letter_set)))\n",
        "    print(\"ຄ່າ sample_idx ທີ່ຊຸມໄດ້: \" + str(sample_idx))\n",
        "                                                         # ຊຸ່ມເອົາເລກໃດໜຶ່ງ ແຕ່ 0 ຫາ 52912 = len(letter_set) ກໍ່ຄືຊຸມເອົາຮູບໃດໜຶ່ງໃນ train_datasets[8]\n",
        "    # extract a 2D slice    \n",
        "    sample_image = letter_set[sample_idx, :, :]          # ໂລດຮູບໂຕທີ່ sample_idx ໃສ່ sample_image ເຊິ່ງເປັນຮູບ 2D \n",
        "\n",
        "    ## sample_image = letter_set[7, :, :]          # ຖ້າບໍ່ຢາກຊຸມກໍ່ໃຊ້ແຖວນີ້ກໍ່ໄດ້ \n",
        "\n",
        "    plt.figure()\n",
        "    \n",
        "    # display it ສະແດງຮູບເພື່ອກວດເບິ່ງວ່າຮູບທີ່ໄດ້ມາມັນດີບໍ່\n",
        "    plt.imshow(sample_image)\n",
        "                    \n",
        "    print(sample_image)   # ເປັນຫຍັງຕົວເລກພວກນີ້ມີຄ່າດຽວຢູ່ໃນພິກເຊວເມື່ອສ້າງເປັນຮູບຄືບໍ່ເປັນຮູບຂາວດຳ"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ຄວາມກວ້າງຂອງ len(letter_set): 52912\n",
            "ຄ່າ sample_idx ທີ່ຊຸມໄດ້: 29752\n",
            "[[-0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.5\n",
            "  -0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.47\n",
            "   0.07  0.3  -0.15 -0.5 ]\n",
            " [-0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.5\n",
            "  -0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.49 -0.48 -0.5  -0.5\n",
            "  -0.5  -0.36  0.41 -0.35]\n",
            " [-0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.5\n",
            "  -0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.49 -0.5  -0.5  -0.5  -0.38\n",
            "  -0.15  0.23  0.5  -0.19]\n",
            " [-0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.5\n",
            "  -0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.49 -0.5  -0.38 -0.01  0.26  0.43\n",
            "   0.5   0.5   0.01 -0.5 ]\n",
            " [-0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.49 -0.48\n",
            "  -0.49 -0.49 -0.48 -0.49 -0.5  -0.49 -0.5  -0.16  0.44  0.5   0.5   0.5\n",
            "   0.28 -0.32 -0.36 -0.08]\n",
            " [-0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.49 -0.5  -0.5  -0.5\n",
            "  -0.5  -0.5  -0.5  -0.5  -0.48 -0.5  -0.37  0.47  0.5   0.5   0.45 -0.03\n",
            "  -0.5  -0.28  0.47 -0.15]\n",
            " [-0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.49 -0.5  -0.5  -0.31 -0.04\n",
            "   0.14  0.15 -0.03 -0.41 -0.5  -0.5  -0.03  0.5   0.5   0.28 -0.32 -0.5\n",
            "  -0.15  0.49  0.28 -0.5 ]\n",
            " [-0.5  -0.5  -0.5  -0.5  -0.5  -0.49 -0.49 -0.5  -0.32  0.19  0.49  0.5\n",
            "   0.5   0.5   0.5   0.46 -0.33 -0.5   0.23  0.5   0.38 -0.42 -0.5  -0.04\n",
            "   0.5   0.5  -0.08 -0.09]\n",
            " [-0.5  -0.5  -0.5  -0.5  -0.48 -0.5  -0.5  -0.07  0.46  0.5   0.49  0.5\n",
            "   0.46  0.16 -0.1   0.1   0.07 -0.47  0.41  0.5  -0.03 -0.5  -0.04  0.5\n",
            "   0.5   0.2  -0.01  0.34]\n",
            " [-0.5  -0.49 -0.49 -0.5  -0.5  -0.39  0.18  0.5   0.49  0.5   0.5   0.2\n",
            "  -0.32 -0.5  -0.5  -0.5  -0.13 -0.26  0.49  0.48 -0.4  -0.25  0.5   0.49\n",
            "   0.5  -0.17  0.37  0.33]\n",
            " [-0.23 -0.5  -0.5  -0.48 -0.13  0.39  0.5   0.5   0.5   0.41 -0.11 -0.5\n",
            "  -0.5  -0.49 -0.48 -0.38 -0.46 -0.09  0.49  0.26 -0.49  0.37  0.49  0.5\n",
            "   0.17 -0.1   0.5   0.28]\n",
            " [-0.22 -0.11  0.13  0.41  0.5   0.5   0.5   0.39  0.05 -0.41 -0.5  -0.49\n",
            "  -0.49 -0.5  -0.46 -0.05 -0.25  0.34  0.5  -0.17 -0.04  0.5   0.48  0.48\n",
            "  -0.24  0.34  0.5   0.29]\n",
            " [-0.5  -0.34 -0.11  0.07  0.06 -0.1  -0.26 -0.44 -0.5  -0.5  -0.49 -0.5\n",
            "  -0.5  -0.49 -0.5  -0.19  0.34  0.43 -0.   -0.3   0.46  0.48  0.5   0.08\n",
            "  -0.12  0.5   0.5   0.3 ]\n",
            " [-0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.48 -0.5  -0.5  -0.5\n",
            "  -0.5  -0.5  -0.5  -0.5  -0.5  -0.48 -0.25  0.42  0.5   0.49  0.43 -0.32\n",
            "   0.37  0.5   0.5   0.3 ]\n",
            " [-0.5  -0.5  -0.5  -0.48 -0.46 -0.48 -0.5  -0.5  -0.5  -0.49 -0.5  -0.5\n",
            "  -0.5  -0.5  -0.5  -0.5  -0.5  -0.48 -0.31  0.31  0.5   0.49  0.45 -0.28\n",
            "   0.32  0.5   0.5   0.3 ]\n",
            " [-0.47 -0.14  0.19  0.36  0.38  0.31  0.14 -0.17 -0.47 -0.5  -0.49 -0.49\n",
            "  -0.5  -0.49 -0.5  -0.23  0.31  0.43  0.03 -0.4   0.41  0.48  0.5   0.17\n",
            "  -0.14  0.5   0.5   0.3 ]\n",
            " [-0.05 -0.01  0.14  0.37  0.5   0.5   0.5   0.5   0.33 -0.17 -0.5  -0.5\n",
            "  -0.48 -0.49 -0.46 -0.03 -0.26  0.33  0.5  -0.11 -0.11  0.5   0.48  0.5\n",
            "  -0.16  0.3   0.5   0.29]\n",
            " [-0.26 -0.5  -0.5  -0.48 -0.17  0.33  0.5   0.48  0.5   0.5   0.17 -0.37\n",
            "  -0.5  -0.5  -0.48 -0.45 -0.4  -0.14  0.49  0.31 -0.47  0.37  0.5   0.5\n",
            "   0.28 -0.08  0.5   0.29]\n",
            " [-0.5  -0.48 -0.49 -0.5  -0.5  -0.45  0.05  0.5   0.5   0.48  0.5   0.43\n",
            "  -0.03 -0.41 -0.5  -0.45 -0.03 -0.3   0.5   0.48 -0.42 -0.25  0.5   0.48\n",
            "   0.5  -0.03  0.35  0.34]\n",
            " [-0.5  -0.5  -0.5  -0.5  -0.49 -0.5  -0.5  -0.25  0.31  0.5   0.5   0.5\n",
            "   0.5   0.41  0.24  0.42 -0.09 -0.46  0.45  0.5  -0.11 -0.5  -0.05  0.5\n",
            "   0.5   0.34  0.06  0.33]\n",
            " [-0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.49 -0.5  -0.47 -0.08  0.29  0.47\n",
            "   0.5   0.5   0.5   0.28 -0.46 -0.5   0.28  0.5   0.35 -0.43 -0.5  -0.03\n",
            "   0.5   0.5   0.06 -0.19]\n",
            " [-0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.49 -0.5  -0.5  -0.5  -0.34\n",
            "  -0.19 -0.16 -0.26 -0.49 -0.5  -0.5  -0.01  0.5   0.5   0.26 -0.37 -0.5\n",
            "  -0.15  0.44  0.41 -0.5 ]\n",
            " [-0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.49 -0.5  -0.5\n",
            "  -0.5  -0.5  -0.5  -0.5  -0.49 -0.5  -0.37  0.47  0.5   0.5   0.4  -0.1\n",
            "  -0.5  -0.39  0.34 -0.04]\n",
            " [-0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.49\n",
            "  -0.49 -0.48 -0.49 -0.5  -0.5  -0.49 -0.5  -0.15  0.45  0.5   0.5   0.5\n",
            "   0.22 -0.35 -0.47 -0.18]\n",
            " [-0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.5\n",
            "  -0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.49 -0.5  -0.37 -0.01  0.25  0.43\n",
            "   0.5   0.5   0.   -0.5 ]\n",
            " [-0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.5\n",
            "  -0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.49 -0.5  -0.5  -0.5  -0.39\n",
            "  -0.19  0.23  0.5  -0.21]\n",
            " [-0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.5\n",
            "  -0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.49 -0.48 -0.5  -0.5\n",
            "  -0.5  -0.4   0.41 -0.32]\n",
            " [-0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.5\n",
            "  -0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.47\n",
            "   0.08  0.25 -0.17 -0.5 ]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_Gdovs6X2W1",
        "colab_type": "text"
      },
      "source": [
        "# Problem 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "deletable": true,
        "editable": true,
        "id": "cYznx5jUwzoO"
      },
      "source": [
        "---\n",
        "Problem 3\n",
        "---------\n",
        "Another check: we expect the data to be balanced across classes. Verify that. 'balanced across classes' ເພື່ອຫຼຸດໂອກາດເກີດຄວາມຜິດພາດໃນການ Training ເຮົາຄວນໃຫ້ມີຈຳນວນສົມດຸ່ນກັນລະຫວ່າງແຕ່ລະ classes ແຕ່ A ຮອດ J\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "deletable": true,
        "editable": true,
        "id": "LA7M7K22ynCt"
      },
      "source": [
        "Merge and prune the training data as needed. Depending on your computer setup, you might not be able to fit it all in memory, and you can tune `train_size` as needed. The labels will be stored into a separate array of integers 0 through 9.\n",
        "\n",
        "Also create a validation dataset for hyperparameter tuning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "deletable": true,
        "editable": true,
        "id": "GPTCnjIcyuKN"
      },
      "source": [
        "Next, we'll randomize the data. It's important to have the labels well shuffled for the training and test distributions to match.\n",
        "ທີ່ຜ່ານມາໄດ້ shuffle ຮູບໃນແຕ່ລະກຸ່ມຕົວອັກສອນ ແລ້ວຈຶ່ງໂລດໃສ່ valid_dataset, valid_labels, train_dataset, train_labels, test_dataset, ແລະ test_labels ເມື່ອໂລດໃສ່ແລ້ວພວກມັນຍັງແຍກເປັນຊັ້ນແຕ່ລະຕົວອັກສອນ ສະນັ້ນເພື່ອໃຫ້ໃກ້ກັບຕົວຈິງຫຼາຍຂື້ນກໍ່ຕ້ອງ random ລຳດັບທັງ່ໝົດໃໝ່ໝົດຂອງ valid_dataset, valid_labels, train_dataset, train_labels, test_dataset, ແລະ test_labelsໂດຍການຈັບຄູ່ຂອງຂໍ້ມູນ ແລະ ລາບິວຂອງຖືກຄູ່ກັນຢູ່"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "colab_type": "code",
        "deletable": true,
        "editable": true,
        "id": "s3mWgZLpyuzq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "7511b85c-8a52-49ed-cdfa-15b73bdbf19d"
      },
      "source": [
        "def make_arrays(nb_rows, img_size):\n",
        "  if nb_rows:\n",
        "    dataset = np.ndarray((nb_rows, img_size, img_size), dtype=np.float32)\n",
        "    labels = np.ndarray(nb_rows, dtype=np.int32)\n",
        "  else:\n",
        "    dataset, labels = None, None\n",
        "  return dataset, labels\n",
        "\n",
        "def merge_datasets(pickle_files, train_size, valid_size=0):    # ສຳລັບຄ່າ valid_size=0 ມັນຈະເທົ່າ 0 ຖ້າໂຕເອີ້ນຟັງຊັນນີ້ບໍ່ໃຫ້ຄ່າ valid_size  \n",
        "# ຟັງຊັນນີ້ລວມ\n",
        "###                  ອະທິບາຍໂຕຫຍໍ້ \n",
        "#   _v ໝາຍເຖິງ _validation\n",
        "#   _t ໝາຍເຖິງ _training\n",
        "#   _l ໝາຍເຖິງ All ຄືຈຳນວນ _v + _t ກໍ່ຄຳ ( end_l = vsize_per_class+tsize_per_class )\n",
        "    \n",
        "  num_classes = len(pickle_files)                              # ເອົາເລກທີ່ເທົ່າກັບຈຳນວນ list ພາຍໃນຕົວປ່່ຽນປະເພດ List ກໍ່ຄື 10 lists ຂອງ pickle_files\n",
        "  print(\"ຄວາມກວ້າງຂອງ ຫຼື ຈຳນວນ list ຂອງ pickle_files: \" + str(len(pickle_files)))\n",
        "\n",
        "  valid_dataset, valid_labels = make_arrays(valid_size, image_size)\n",
        "  train_dataset, train_labels = make_arrays(train_size, image_size)\n",
        "  vsize_per_class = valid_size // num_classes\n",
        "  tsize_per_class = train_size // num_classes\n",
        "    \n",
        "  start_v, start_t = 0, 0\n",
        "  end_v, end_t = vsize_per_class, tsize_per_class\n",
        "  end_l = vsize_per_class+tsize_per_class\n",
        "  for label, pickle_file in enumerate(pickle_files):     # enumerate ສ້າງເລກລຳດັບຈັບຄູ່ກັບແຕ່ລະອົງປະກອບ ກໍ່ຄືລຳດັບ 0 ແທນໃຫ້ອົງປະກອບທຳອິດ\n",
        "    try:                                                 # ໝາຍຄວາມວ່າລຳດັບ 0 ແທນໃຫ້ຕົວອັກສອນ A ເພາະ A ແມ່ນອົງປະກອບທຳອິດຂອງ pickle_files\n",
        "      with open(pickle_file, 'rb') as f:\n",
        "        letter_set = pickle.load(f)\n",
        "        # let's shuffle the letters to have random  and training set\n",
        "        np.random.shuffle(letter_set)         # shuffle ເຮັດໃຫ້ຈັດລຳດັບໃໝ່ແບບຊຸ່ມເດົາພາຍໃນ letter_set ກໍ່ຄືພາຍໃນກຸ່ມຕົວອັກສອນຂອງມັນ ຕົວຢ່າງ https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.random.shuffle.html\n",
        "        \n",
        "        # ຫຼັງຈາກຈັດລຳດັບໃໝ່ແບບຊຸ່ມເດົາແລ້ວຈຶ່ງປ້ອນຮູບໃສ່  valid_dataset ແລະ ໃສ່ train_dataset ໂດຍເອົາແຕ່ລຳດັບ 0(ຕົວອັກສອນ A) ຈົນໝົດຕໍ່ດ້ວຍ ລຳດັບ 1(ຕົວອັກສອນ B) ສຸດທ້າຍຮອດ ລຳດັບ 9(ຕົວອັກສອນ J)\n",
        "        # ແລ້ວກໍ່ຈະໄດ້ valid_dataset, valid_labels, train_dataset, train_labels ທີ່ມີທຸກຕົວອັກສອນ ພ້ອມລຽນລຳດັບຈັບຄູ່ກັບ label ຂອງມັນເອງ ກໍ່ຄື valid_dataset ຄູ່ກັບ valid_labels ແລະ ຄູ່ອື່ນໆ\n",
        "        if valid_dataset is not None:\n",
        "          valid_letter = letter_set[:vsize_per_class, :, :]    \n",
        "          valid_dataset[start_v:end_v, :, :] = valid_letter\n",
        "          valid_labels[start_v:end_v] = label\n",
        "          start_v += vsize_per_class\n",
        "          end_v += vsize_per_class\n",
        "                    \n",
        "        train_letter = letter_set[vsize_per_class:end_l, :, :]\n",
        "        train_dataset[start_t:end_t, :, :] = train_letter\n",
        "        train_labels[start_t:end_t] = label\n",
        "        start_t += tsize_per_class\n",
        "        end_t += tsize_per_class\n",
        "    except Exception as e:\n",
        "      print('Unable to process data from', pickle_file, ':', e)\n",
        "      raise\n",
        "###  print('label :', label)\n",
        "###  print('vsize_per_class: ',vsize_per_class)\n",
        "\n",
        "  return valid_dataset, valid_labels, train_dataset, train_labels\n",
        "            \n",
        "train_size = 200000      # ຂະໜາດຈາກ ຕົວຢ່າງເຂົາເຈົ້າ\n",
        "valid_size = 10000\n",
        "test_size = 10000\n",
        "  \n",
        "## train_size = 2000 * 4  # ເນື່ອງຈາກຈຳນວນຝຶກແອບມີຮອດປະມານ 500000ໂຕ ແຕ່ຄອມຂ້ອຍສາມາດຝຶກແອບໄດ້ປະມານພຽງ 10000 ໂຕຝຶກແອບ\n",
        "## valid_size = 150 * 4   # ຂະໜາດຂໍ້ມູນທີ່ໃຊ້ທົດລອງ\n",
        "## test_size = 100 * 4\n",
        "\n",
        "# ຄຳສັ່ງແຖວຂ້າງລູ່ມນີ້ເລືອກເອົາບັນດາຕົວຝຶກແອບອອກເປັນ 2ກຸ່ມຄື: ກຸ່ມຝຶກແອບ ແລະ ກຸ່ມທົດລອງ ຕາມກຳນົດຂ້າງເທິງ ໂດຍເລືອກເອົາແບບຊຸ່ມເອົາ 'shuffle'\n",
        "valid_dataset, valid_labels, train_dataset, train_labels = merge_datasets(\n",
        "  train_datasets, train_size, valid_size)         \n",
        "\n",
        "_, _, test_dataset, test_labels = merge_datasets(test_datasets, test_size)\n",
        "\n",
        "print('Training:', train_dataset.shape, train_labels.shape)\n",
        "print('Validation:', valid_dataset.shape, valid_labels.shape)\n",
        "print('Testing:', test_dataset.shape, test_labels.shape)\n",
        "\n",
        "### 200510"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ຄວາມກວ້າງຂອງ ຫຼື ຈຳນວນ list ຂອງ pickle_files: 10\n",
            "ຄວາມກວ້າງຂອງ ຫຼື ຈຳນວນ list ຂອງ pickle_files: 10\n",
            "Training: (200000, 28, 28) (200000,)\n",
            "Validation: (10000, 28, 28) (10000,)\n",
            "Testing: (10000, 28, 28) (10000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "colab_type": "code",
        "deletable": true,
        "editable": true,
        "id": "6WZ2l2tN2zOL",
        "colab": {}
      },
      "source": [
        "def randomize(dataset, labels):    \n",
        "  permutation = np.random.permutation(labels.shape[0])   # ເປັນຄຳສັງສະລັບລຳດັບຂອງອົງປະກອບແບບຊຸມ ໃນແຖວນີ້ຈະສະລັບເລກລຳດັບແຕ່ 0 ຫາ ລຳດັບໃຫຍ່ສຸດ\n",
        "                                                         # ຂອງ labels ກໍ່ຄືເລກ labels.shape[0]\n",
        "                                                         # np.random.permutation() Randomly permute a sequence, or return a permuted range.\n",
        "                                                         # If x is a multi-dimensional array, it is only shuffled along its first index.\n",
        "  shuffled_dataset = dataset[permutation,:,:]\n",
        "  shuffled_labels = labels[permutation]\n",
        "  return shuffled_dataset, shuffled_labels\n",
        "\n",
        "  \n",
        "train_dataset, train_labels = randomize(train_dataset, train_labels)\n",
        "test_dataset, test_labels = randomize(test_dataset, test_labels)\n",
        "valid_dataset, valid_labels = randomize(valid_dataset, valid_labels)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "deletable": true,
        "editable": true,
        "id": "puDUTe6t6USl"
      },
      "source": [
        "---\n",
        "Problem 4\n",
        "---------\n",
        "Convince yourself that the data is still good after shuffling!\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "deletable": true,
        "editable": true,
        "id": "tIQJaJuwg5Hw"
      },
      "source": [
        "Finally, let's save the data for later reuse:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "colab_type": "code",
        "deletable": true,
        "editable": true,
        "id": "QiR_rETzem6C",
        "colab": {}
      },
      "source": [
        "pickle_file = os.path.join(data_root, 'notMNIST.pickle')     # ຟາຍ 'notMNIST.pickle' ເຮັດໜ້າທີ່ເກັບຂໍ້ມູນທັງໝົດໃຊ້ໃຫ້ຄອມຮຽນຮູ້ຕົວອັກສອນ\n",
        "                                                             # ເຊິ່ງຂໍ້ມູນນັ້ນເປັນໄບນາລີ ຢູ່ໃນຕົວປ່ຽນແບບ array\n",
        "try:\n",
        "  f = open(pickle_file, 'wb') \n",
        "  save = {\n",
        "    'train_dataset': train_dataset,\n",
        "    'train_labels': train_labels,\n",
        "    'valid_dataset': valid_dataset,\n",
        "    'valid_labels': valid_labels,\n",
        "    'test_dataset': test_dataset,\n",
        "    'test_labels': test_labels,\n",
        "    }\n",
        "  pickle.dump(save, f, pickle.HIGHEST_PROTOCOL)\n",
        "  f.close()                                                   # ລະສົງໃສວ່າມັນແພັກເອົາໄວ້ເຮັດຫຍັງ ອີ່ແທ້ມັນເອົາໄປໃຊ້ Assignment 3\n",
        "\n",
        "except Exception as e:\n",
        "  print('Unable to save data to', pickle_file, ':', e)\n",
        "  raise"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "colab_type": "code",
        "deletable": true,
        "editable": true,
        "id": "hQbLjrW_iT39",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "026344a0-9fb7-4177-84f9-6ef6554bc7c2"
      },
      "source": [
        "statinfo = os.stat(pickle_file)\n",
        "print('Compressed pickle size:', statinfo.st_size)        ### ຕົວເລກທີ່ມັນສະແດງອອກມາໜ້າຈະແມ່ນ byte\n",
        "\n",
        "###   200510 ນາ    ####\n",
        "###   200421 ນາ    ####\n",
        "###   180101 ນາ    ####"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Compressed pickle size: 690800506\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "deletable": true,
        "editable": true,
        "id": "gE_cRAQB33lk"
      },
      "source": [
        "---\n",
        "Problem 5\n",
        "---------\n",
        "\n",
        "By construction, this dataset might contain a lot of overlapping samples, including training data that's also contained in the validation and test set! Overlap between training and test can skew the results if you expect to use your model in an environment where there is never an overlap, but are actually ok if you expect to see training samples recur when you use it.\n",
        "Measure how much overlap there is between training, validation and test samples.\n",
        "\n",
        "Optional questions:\n",
        "- What about near duplicates between datasets? (images that are almost identical)\n",
        "- Create a sanitized validation and test set, and compare your accuracy on those in subsequent assignments.\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "0zmVGRS95GI8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "89586a58-72b9-4610-e2f3-dbc909b05f66"
      },
      "source": [
        "import time\n",
        "\n",
        "def check_overlaps(images1, images2):\n",
        "    images1.flags.writeable=False\n",
        "    images2.flags.writeable=False\n",
        "    start = time.clock()\n",
        "    hash1 = set([hash(image1.tobytes()) for image1 in images1])\n",
        "    hash2 = set([hash(image2.tobytes()) for image2 in images2])\n",
        "    all_overlaps = set.intersection(hash1, hash2)\n",
        "    return all_overlaps, time.clock()-start\n",
        "\n",
        "r, execTime = check_overlaps(train_dataset, test_dataset)    \n",
        "print('Number of overlaps between training and test sets: {}. Execution time: {}.'.format(len(r), execTime))\n",
        "\n",
        "r, execTime = check_overlaps(train_dataset, valid_dataset)   \n",
        "print('Number of overlaps between training and validation sets: {}. Execution time: {}.'.format(len(r), execTime))\n",
        "\n",
        "r, execTime = check_overlaps(valid_dataset, test_dataset) \n",
        "print('Number of overlaps between validation and test sets: {}. Execution time: {}.'.format(len(r), execTime))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of overlaps between training and test sets: 1123. Execution time: 0.6411429999999996.\n",
            "Number of overlaps between training and validation sets: 928. Execution time: 0.611725999999976.\n",
            "Number of overlaps between validation and test sets: 62. Execution time: 0.05842499999999973.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "deletable": true,
        "editable": true,
        "id": "L8oww1s4JMQx"
      },
      "source": [
        "---\n",
        "Problem 6\n",
        "---------\n",
        "\n",
        "Let's get an idea of what an off-the-shelf classifier can give you on this data. It's always good to check that there is something to learn, and that it's a problem that is not so trivial that a canned solution solves it.\n",
        "\n",
        "Train a simple model on this data using 50, 100, 1000 and 5000 training samples. Hint: you can use the LogisticRegression model from sklearn.linear_model.\n",
        "\n",
        "Optional question: train an off-the-shelf model on all the data!\n",
        "\n",
        "Training: (50, 28, 28) (50,)\n",
        "Validation: (10000, 28, 28) (10000,)\n",
        "Testing: (10000, 28, 28) (10000,)\n",
        "\n",
        "ໃຊ້      50 training samples ແລະ GPU ຜົນໄດ້ຮັບ  0.6864                     ໃຊ້ເວລາ 1.4ວິນາທີ\n",
        "ໃຊ້    100 training samples ແລະ GPU ຜົນໄດ້ຮັບ  0.7786                     ໃຊ້ເວລາ 1.2ວິນາທີ\n",
        "ໃຊ້  1000 training samples ແລະ GPU ຜົນໄດ້ຮັບ  0.8305                     ໃຊ້ເວລາ 2.5ວິນາທີ\n",
        "ໃຊ້  5000 training samples ແລະ GPU ຜົນໄດ້ຮັບ  0.8368                     ໃຊ້ເວລາ 20.1ວິນາທີ\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "9VdgjdgJ5GJB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c786719a-69ed-4a1e-ebea-818e0917c115"
      },
      "source": [
        "# Here you have 200000 samples\n",
        "# 28 x 28 features\n",
        "# We have to reshape them because scikit-learn expects (n_samples, n_features)\n",
        "train_dataset.shape\n",
        "\n",
        "###  print(train_dataset)\n",
        "###  display(train_dataset)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(200000, 28, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "6R0WEADU5GJE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### test_dataset.shape\n",
        "\n",
        "### print(test_dataset)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "a8EiyM275GJH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 840
        },
        "outputId": "d1f20960-00f8-47bc-c5c4-3db215509acd"
      },
      "source": [
        "# Prepare training data\n",
        "samples, width, height = train_dataset.shape\n",
        "\n",
        "print(train_dataset)\n",
        "\n",
        "X_train = np.reshape(train_dataset,(samples,width*height))     ###  ເນື່ອງຈາກ ເຮົາຈະໃຊ້ LogisticRegression model from sklearn.linear_model ເຊິ່ງມັນກຳນົດໃຫ້ ຕ້ອງປ້ອນຂໍ້ມູນແບບ (n_samples, n_features)\n",
        "y_train = train_labels                                         ###  ສະນັ້ນເຂົາຈຶ່ງເປັນຮູບຮ່າງຂໍ້ມູນ ຈາກ 3ມີຕິ ເປັນ 2ມິຕີ ດ້ວຍຄຳສັ່ງ reshape\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Prepare testing data\n",
        "samples, width, height = test_dataset.shape\n",
        "X_test = np.reshape(test_dataset,(samples,width*height))\n",
        "y_test = test_labels"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[-0.5  -0.5  -0.5  ...  0.46  0.5   0.03]\n",
            "  [-0.5  -0.5  -0.5  ...  0.5   0.49 -0.28]\n",
            "  [-0.5  -0.5  -0.5  ...  0.5   0.31 -0.47]\n",
            "  ...\n",
            "  [-0.5  -0.5  -0.5  ... -0.5  -0.5  -0.5 ]\n",
            "  [-0.09  0.21  0.29 ... -0.5  -0.5  -0.5 ]\n",
            "  [ 0.28  0.5   0.46 ... -0.5  -0.5  -0.5 ]]\n",
            "\n",
            " [[-0.5  -0.5  -0.5  ...  0.05 -0.41 -0.5 ]\n",
            "  [-0.5  -0.5  -0.5  ...  0.5   0.22 -0.46]\n",
            "  [-0.5  -0.5  -0.5  ...  0.47  0.5  -0.34]\n",
            "  ...\n",
            "  [-0.5  -0.5  -0.5  ... -0.5  -0.5  -0.5 ]\n",
            "  [-0.5  -0.5  -0.5  ... -0.5  -0.5  -0.5 ]\n",
            "  [-0.5  -0.5  -0.5  ... -0.5  -0.5  -0.5 ]]\n",
            "\n",
            " [[-0.5  -0.5  -0.5  ... -0.49 -0.5  -0.5 ]\n",
            "  [-0.5  -0.5  -0.5  ...  0.31 -0.05 -0.43]\n",
            "  [-0.5  -0.5  -0.5  ...  0.49  0.5   0.01]\n",
            "  ...\n",
            "  [-0.5  -0.5  -0.49 ... -0.5  -0.49 -0.5 ]\n",
            "  [-0.5  -0.5  -0.5  ... -0.5  -0.49 -0.5 ]\n",
            "  [-0.5  -0.5  -0.5  ... -0.5  -0.5  -0.5 ]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-0.5  -0.5  -0.5  ... -0.27 -0.45 -0.5 ]\n",
            "  [-0.5  -0.5  -0.5  ...  0.49  0.39 -0.03]\n",
            "  [-0.5  -0.5  -0.5  ...  0.49  0.5   0.19]\n",
            "  ...\n",
            "  [-0.5  -0.5  -0.5  ...  0.48  0.5   0.02]\n",
            "  [-0.5  -0.5  -0.5  ...  0.33  0.15 -0.3 ]\n",
            "  [-0.5  -0.5  -0.5  ... -0.46 -0.5  -0.5 ]]\n",
            "\n",
            " [[-0.5  -0.5  -0.5  ...  0.49  0.5   0.1 ]\n",
            "  [-0.5  -0.5  -0.5  ...  0.49  0.46  0.01]\n",
            "  [-0.5  -0.5  -0.5  ... -0.37 -0.43 -0.5 ]\n",
            "  ...\n",
            "  [ 0.04  0.5   0.48 ... -0.5  -0.5  -0.5 ]\n",
            "  [-0.02  0.5   0.5  ... -0.5  -0.5  -0.5 ]\n",
            "  [-0.48  0.01  0.06 ... -0.5  -0.5  -0.5 ]]\n",
            "\n",
            " [[-0.5  -0.5  -0.5  ...  0.5   0.4  -0.15]\n",
            "  [-0.5  -0.5  -0.5  ...  0.49  0.5   0.36]\n",
            "  [-0.5  -0.5  -0.5  ...  0.34  0.5   0.33]\n",
            "  ...\n",
            "  [-0.32 -0.1  -0.11 ... -0.5  -0.5  -0.5 ]\n",
            "  [ 0.21  0.5   0.49 ... -0.5  -0.5  -0.5 ]\n",
            "  [ 0.08  0.13  0.12 ... -0.5  -0.5  -0.5 ]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "coDIPuLDoQuy",
        "colab_type": "text"
      },
      "source": [
        "# ຜົນການເຝືກແບບໃຊ້ LogisticRegression form sklearn ໄປປະມານ accuracy score = 0.8997 ຈາກ train dataset = 200,000ຮູບ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "7cmxCy6j5GJK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "a8557d8a-5a35-4b96-dc61-db10560af10d"
      },
      "source": [
        "# Import\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Instantiate\n",
        "lg = LogisticRegression(multi_class='multinomial', solver='lbfgs', random_state=42, verbose=1, max_iter=1000, n_jobs=-1)\n",
        "\n",
        "# Fit   ### Fit the model according to the given training data\n",
        "lg.fit(X_train, y_train)   ###---     ແຖວນີ້ແມ່ນຄຳສັ່ງໃຫ້ເຝິກແອບ(fit the model)     ---###\n",
        "\n",
        "# Predict\n",
        "y_pred = lg.predict(X_test)   ###--- ຫຼັງຈາກຮຽນແລ້ວກໍ່ມາທົດສອບກັບຊຸດຂໍ້ມູນ X_test ຊຶ່ງຜົນການທົດສອບເກັບໄວ້ຢູ່ y_pred       ---###\n",
        "\n",
        "# Score\n",
        "from sklearn import metrics\n",
        "metrics.accuracy_score(y_test, y_pred)    ###--- ຫຼັງຈາກທົດສອບແລ້ວກໍ່ມາເບິ່ງຄວາມແນ່ນອນໂດຍທຽບກັນລະຫວ່າງ y_test ກັບ  y_pred     ---###\n",
        "\n",
        "###   180106 ນາ    ####"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed: 24.4min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8969"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "D-CUMOIQ5GJN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###   180603 ສືກສາຄືນເປັນຄັ້ງທີ່ 2  ຜົນໄດ້ຮັບໂອກາດແທ້ຕ້ອງແມ່ນ 86.79999999%  ####"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddj_y5v5ZW4G",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzOcCXzNF-RO",
        "colab_type": "text"
      },
      "source": [
        "190505\n",
        "\n",
        "Training: (10000, 28, 28) (10000,)\n",
        "Validation: (750, 28, 28) (750,)\n",
        "Testing: (500, 28, 28) (500,)\n",
        "\n",
        "ໃຊ້ CPU ຜົນໄດ້ຮັບ  0.862(86.2%)     ໃຊ້ເວລາ 55.4ວິນາທີ\n",
        "ໃຊ້ GPU ຜົນໄດ້ຮັບ  0.89                     ໃຊ້ເວລາ 44.9ວິນາທີ\n",
        "\n",
        "\n",
        "Training: (200000, 28, 28) (200000,)\n",
        "Validation: (10000, 28, 28) (10000,)\n",
        "Testing: (10000, 28, 28) (10000,)\n",
        "\n",
        "ໃຊ້ CPU ຜົນໄດ້ຮັບ 0.8967               ໃຊ້ເວລາ   18.4ນາທີ            physical_device_desc: \"device: XLA_CPU device\"\n",
        "ໃຊ້ GPU ຜົນໄດ້ຮັບ 0.898                 ໃຊ້ເວລາ   15.0ນາທີ            physical_device_desc: \"device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\"\n",
        "\n",
        "190513\n",
        "\n",
        "Training: (2000, 28, 28) (2000,)\n",
        "Validation: (100, 28, 28) (100,)\n",
        "Testing: (100, 28, 28) (100,)\n",
        "\n",
        "ໃຊ້ GPU ຜົນໄດ້ຮັບ  0.91                     ໃຊ້ເວລາ 4.8ວິນາທີ           device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iglIFGEEZjYK",
        "colab_type": "text"
      },
      "source": [
        "200421\n",
        "\n",
        "ໃຊ້ CPU ຢູ່ ຄອມ 7 Training (200000,28,28) Validation (10000), Testing (10000) ຜົນໄດ້ຮັບ 0.8993 ໃຊ້ເວລາ 27.1 ນາທີ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7FDYbN4ZbyC",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYNoebzEYz40",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        },
        "outputId": "736ce81c-7d3e-43d4-fb76-6dd37279d651"
      },
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "\n",
        "print(\"Show System RAM Memory:\\n\\n\")\n",
        "!cat /proc/meminfo | egrep \"MemTotal*\"\n",
        "\n",
        "\n",
        "print(\"\\n\\nShow Devices:\\n\\n\"+str(device_lib.list_local_devices()))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Show System RAM Memory:\n",
            "\n",
            "\n",
            "MemTotal:       13333552 kB\n",
            "\n",
            "\n",
            "Show Devices:\n",
            "\n",
            "[name: \"/device:CPU:0\"\n",
            "device_type: \"CPU\"\n",
            "memory_limit: 268435456\n",
            "locality {\n",
            "}\n",
            "incarnation: 7104047907140843852\n",
            ", name: \"/device:XLA_CPU:0\"\n",
            "device_type: \"XLA_CPU\"\n",
            "memory_limit: 17179869184\n",
            "locality {\n",
            "}\n",
            "incarnation: 1458202110190404816\n",
            "physical_device_desc: \"device: XLA_CPU device\"\n",
            "]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTfCBC0L_Jto",
        "colab_type": "text"
      },
      "source": [
        "# ທົດລອງລັນ DL Assignment 3 Regularization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7y05D8WT_WT-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "b5c3e8df-ffd7-4614-b597-610b3d451830"
      },
      "source": [
        "### ລຳດັບເຮັດ Assignment 3 https://github.com/tensorflow/examples/blob/master/courses/udacity_deep_learning/3_regularization.ipynb\n",
        "\n",
        "### ຄຳຕອບຂອງ Assignment ນີ້: https://www.ritchieng.com/machine-learning/deep-learning/tensorflow/regularization/\n",
        "\n",
        "\n",
        "# These are all the modules we'll be using later. Make sure you can import them\n",
        "# before proceeding further.\n",
        "from __future__ import print_function\n",
        "import numpy as np\n",
        "\n",
        "###!pip install tensorflow==2.2\n",
        "\n",
        "%tensorflow_version 1.x\n",
        "\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "print(tf.keras.__version__)\n",
        "\n",
        "# import tensorflow as tf\n",
        "\n",
        "\n",
        "from six.moves import cPickle as pickle"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.15.2\n",
            "2.2.4-tf\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXIO52VC_cqk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "88c7397a-0c2c-440c-9c45-c2e57c39f000"
      },
      "source": [
        "pickle_file = 'notMNIST.pickle'\n",
        "\n",
        "with open(pickle_file, 'rb') as f:\n",
        "  save = pickle.load(f)\n",
        "  train_dataset = save['train_dataset']\n",
        "  train_labels = save['train_labels']\n",
        "  valid_dataset = save['valid_dataset']\n",
        "  valid_labels = save['valid_labels']\n",
        "  test_dataset = save['test_dataset']\n",
        "  test_labels = save['test_labels']\n",
        "  del save  # hint to help gc free up memory\n",
        "  print('Training set', train_dataset.shape, train_labels.shape)\n",
        "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
        "  print('Test set', test_dataset.shape, test_labels.shape)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training set (200000, 28, 28) (200000,)\n",
            "Validation set (10000, 28, 28) (10000,)\n",
            "Test set (10000, 28, 28) (10000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65nlGomiDojz",
        "colab_type": "text"
      },
      "source": [
        "Reformat into a shape that's more adapted to the models we're going to train:\n",
        "\n",
        "data as a flat matrix,\n",
        "\n",
        "labels as float 1-hot encodings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-mR5xY8yDqRV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "7810edd2-a193-45ae-8e34-59aa92fba6cf"
      },
      "source": [
        "image_size = 28\n",
        "num_labels = 10\n",
        "\n",
        "def reformat(dataset, labels):\n",
        "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
        "  # Map 2 to [0.0, 1.0, 0.0 ...], 3 to [0.0, 0.0, 1.0 ...]\n",
        "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
        "  return dataset, labels\n",
        "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
        "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
        "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
        "print('Training set(shape of train dataset and train label :', train_dataset.shape, train_labels.shape)\n",
        "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
        "print('Test set', test_dataset.shape, test_labels.shape)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training set(shape of train dataset and train label : (200000, 784) (200000, 10)\n",
            "Validation set (10000, 784) (10000, 10)\n",
            "Test set (10000, 784) (10000, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTMbIbuVEHoF",
        "colab_type": "text"
      },
      "source": [
        "Problem 1\n",
        "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor t using nn.l2_loss(t). The right amount of regularization should improve your validation / test accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLiaJaYKEJzc",
        "colab_type": "text"
      },
      "source": [
        "Multinomial logistic regression with L2 loss function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DwVReuwEPLs",
        "colab_type": "text"
      },
      "source": [
        "![image.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WbRmazbwEd_F",
        "colab_type": "text"
      },
      "source": [
        "1. Load Data & Build Computation Graph\n",
        "\n",
        "   ຂັ້ນຕອນການສ້າງອົງປະກອບ ແລະ ໃຫ້ຄ່າຕ່າງໆ ຂອງສົມຜົນ ກໍ່ຄືໃຫ້ຄ່າຕ່າງໆຂອງໂຄງຮ່າງການປະເມີນຜົນ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fUwRSHoIEYHl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "fd6acb72-c219-4255-8959-de0594859a07"
      },
      "source": [
        "# This is to expedite the process \n",
        "### ຢູຸ່ Assignment 1 ໃຊ້ train dataset 200,000 ຮູບ ແຕ່ ຢູ່ ນີ້ໃຊ້ ພຽງ train_subset = 10,000 ຮູບ\n",
        "train_subset = 10000\n",
        "# This is a good beta value to start with\n",
        "beta = 0.01\n",
        "\n",
        "graph = tf.Graph()\n",
        "with graph.as_default():\n",
        "\n",
        "    # Input data.\n",
        "    # They're all constants.\n",
        "    tf_train_dataset = tf.constant(train_dataset[:train_subset, :])\n",
        "    print('ຢູຸ່ Assignment 1 ໃຊ້ train dataset 200,000 ຮູບ ແຕ່ ຢູ່ ນີ້ໃຊ້ ພຽງ train_subset = ',tf_train_dataset.shape[0],' ຮູບ' )\n",
        "    print('ມື້ໜ້າລອງປ່ຽນຈຳນວນເບິ່ງ ເພາະສົ່ງໃສ Assignment 1 ໄດ້ 86.79999999% ແຕ່ ຂັ້ນຕອນ num_steps = 801 ໄດ້ປະມານ 89.0 ຄືບໍ່ຕ່າງກັນຫຼາຍທັງທີ່ເປັນວີທີໃໝ່')\n",
        "    print('ແຕ່ກໍ່ເປັນການໃຊ້ຈຳນວນຂໍ້ມູນໜ້ອຍກວ່າຫຼາຍ ຈາກ 200,000 ຮູບ ເປັນ 10,000 ຮູຸບ')\n",
        "\n",
        "    tf_train_labels = tf.constant(train_labels[:train_subset])\n",
        "    print('tf_train_labels :', tf_train_labels.shape)\n",
        "    \n",
        "    tf_valid_dataset = tf.constant(valid_dataset)\n",
        "    tf_test_dataset = tf.constant(test_dataset)\n",
        "  \n",
        "    # Variables    \n",
        "    # They are variables we want to update and optimize.\n",
        "    weights = tf.Variable(tf.truncated_normal([image_size * image_size, num_labels]))\n",
        "\n",
        "# ຖ້າເກີດ Error when running tensorflow in virtualenv: module 'tensorflow' has no attribute 'truncated_normal'\n",
        "# ໃຫ້ແກ້ຕາມ https://stackoverflow.com/questions/58341433/error-when-running-tensorflow-in-virtualenv-module-tensorflow-has-no-attribut\n",
        "# ມັນຈະວ່າ Keras 2.2.4 does not support TensorFlow 2.0 (it was released much before TF 2.0),\n",
        "# so you can either downgrade TensorFlow to version 1.x,\n",
        "# or upgrade Keras to version 2.3, which does support TensorFlow 2.0.\n",
        "# ເຊິ່ງຂ້ອຍໃຊ້ downgrade TensorFlow to version 1.x,\n",
        "# ດ້ວຍ   %tensorflow_version 1.x\n",
        "#        import tensorflow as tf\n",
        "#        print(tf.__version__)\n",
        "\n",
        "\n",
        "    biases = tf.Variable(tf.zeros([num_labels]))\n",
        "  \n",
        "    # Training computation.\n",
        "    logits = tf.matmul(tf_train_dataset, weights) + biases \n",
        "    ###  ເລື້ມທຳອິດ weights ມີຄ່າແບບ ramdom ສ່ວນ biases = 0    \n",
        "    # Original loss function\n",
        "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = tf_train_labels) )\n",
        "    ### ຢູ່   Assignment1 ບໍ່ມີ loss ມີ accuracy\n",
        "\n",
        "    # Loss function using L2 Regularization\n",
        "    regularizer = tf.nn.l2_loss(weights)\n",
        "    loss = tf.reduce_mean(loss + beta * regularizer)\n",
        "    \n",
        "    # Optimizer.\n",
        "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
        "  \n",
        "    # Predictions for the training, validation, and test data.   ??? ມັນຫຍັງຕ້ອງ ຄາດເດົາຄ່າພວກນີ້ນຳ train_prediction \n",
        "    train_prediction = tf.nn.softmax(logits)\n",
        "    valid_prediction = tf.nn.softmax( tf.matmul(tf_valid_dataset, weights) + biases )\n",
        "    test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ຢູຸ່ Assignment 1 ໃຊ້ train dataset 200,000 ຮູບ ແຕ່ ຢູ່ ນີ້ໃຊ້ ພຽງ train_subset =  10000  ຮູບ\n",
            "ມື້ໜ້າລອງປ່ຽນຈຳນວນເບິ່ງ ເພາະສົ່ງໃສ Assignment 1 ໄດ້ 86.79999999% ແຕ່ ຂັ້ນຕອນ num_steps = 801 ໄດ້ປະມານ 89.0 ຄືບໍ່ຕ່າງກັນຫຼາຍທັງທີ່ເປັນວີທີໃໝ່\n",
            "ແຕ່ກໍ່ເປັນການໃຊ້ຈຳນວນຂໍ້ມູນໜ້ອຍກວ່າຫຼາຍ ຈາກ 200,000 ຮູບ ເປັນ 10,000 ຮູຸບ\n",
            "tf_train_labels : (10000, 10)\n",
            "WARNING:tensorflow:From <ipython-input-26-5bea81b9e7bc>:44: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-K1955USECmU",
        "colab_type": "text"
      },
      "source": [
        "1. Run Computation & Iterate\n",
        "\n",
        "   ຂັ້ນຕອນການປະຕິບັດ ແລະ ຈະປະຕິບັດຊຳ້ອີກ ຂອງສົມຜົນທີ່ໄດ້ສ້າງມາ\n",
        "   ເພື່ອເບິ່ງການປ່ຽນແປງຂອງ loss ແລະ accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUjWoShDEGQX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 574
        },
        "outputId": "9e1b3961-571d-462b-9f63-4b25fd901542"
      },
      "source": [
        "num_steps = 801\n",
        "\n",
        "def accuracy(predictions, labels):\n",
        "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
        "            / predictions.shape[0])\n",
        "\n",
        "with tf.Session(graph=graph) as session:\n",
        "    # This is a one-time operation which ensures the parameters get initialized as\n",
        "    # we described in the graph: random weights for the matrix, zeros for the\n",
        "    # biases.       \n",
        "    tf.initialize_all_variables().run()\n",
        "    print('Initialized')\n",
        "    for step in range(num_steps):\n",
        "    # Run the computations. We tell .run() that we want to run the optimizer,\n",
        "    # and get the loss value and the training predictions returned as numpy\n",
        "    # arrays. \n",
        "        _, l, predictions = session.run([optimizer, loss, train_prediction])\n",
        "### ຮ່າໆໆ ເບິ່ງໄປເບິ່ງມາກະເປັນຕາຫົວເນາະ ລັນຢູ່ແຖວດຽວນີ້ 800 ເທື່ອ ເປັນຄວາມສາມາດຂອງ tf\n",
        "### ແລ້ວມັນລັນໂຕໃດກ່ອນ ລະຫວ່າງ optimizer, loss ແລະ train_prediction\n",
        "\n",
        "### ອັນຂີດກ້ອງນີ້ _, ຄວາມໝາຍມັນແມ່ນຫຍັງນໍ?        \n",
        "        if (step % 100 == 0):\n",
        "            print('Loss at step {}: {}'.format(step, l))\n",
        "            print('Training accuracy: {:.2f}'.format(accuracy(predictions, \n",
        "                                                         train_labels[:train_subset, :])))\n",
        "            # Calling .eval() on valid_prediction is basically like calling run(), but\n",
        "            # just to get that one numpy array. Note that it recomputes all its graph\n",
        "            # dependencies.\n",
        "            \n",
        "            # You don't have to do .eval above because we already ran the session for the\n",
        "            # train_prediction\n",
        "            print('Validation accuracy: {:.1f}'.format(accuracy(valid_prediction.eval(), \n",
        "                                                           valid_labels)))\n",
        "    print('Test accuracy: {:.1f}'.format(accuracy(test_prediction.eval(), test_labels)))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/util/tf_should_use.py:198: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
            "Instructions for updating:\n",
            "Use `tf.global_variables_initializer` instead.\n",
            "Initialized\n",
            "Loss at step 0: 47.683937072753906\n",
            "Training accuracy: 9.24\n",
            "Validation accuracy: 11.4\n",
            "Loss at step 100: 11.494377136230469\n",
            "Training accuracy: 73.52\n",
            "Validation accuracy: 72.3\n",
            "Loss at step 200: 4.362926959991455\n",
            "Training accuracy: 78.34\n",
            "Validation accuracy: 76.7\n",
            "Loss at step 300: 1.9463412761688232\n",
            "Training accuracy: 81.41\n",
            "Validation accuracy: 79.8\n",
            "Loss at step 400: 1.1272419691085815\n",
            "Training accuracy: 82.91\n",
            "Validation accuracy: 81.2\n",
            "Loss at step 500: 0.8461117148399353\n",
            "Training accuracy: 83.51\n",
            "Validation accuracy: 81.6\n",
            "Loss at step 600: 0.7484318017959595\n",
            "Training accuracy: 83.73\n",
            "Validation accuracy: 81.7\n",
            "Loss at step 700: 0.7141733765602112\n",
            "Training accuracy: 83.84\n",
            "Validation accuracy: 81.8\n",
            "Loss at step 800: 0.7020689249038696\n",
            "Training accuracy: 83.93\n",
            "Validation accuracy: 81.7\n",
            "Test accuracy: 89.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4tLUVI0EOX2",
        "colab_type": "text"
      },
      "source": [
        "Neural Network with L2 Regularization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mG_Ely85EPjI",
        "colab_type": "text"
      },
      "source": [
        "1 Hidden Layer using RELUs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JqgRDMyHEs0k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_nodes= 1024\n",
        "batch_size = 128\n",
        "beta = 0.01\n",
        "\n",
        "graph = tf.Graph()\n",
        "with graph.as_default():\n",
        "\n",
        "    # Input data. For the training data, we use a placeholder that will be fed\n",
        "    # at run time with a training minibatch.\n",
        "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size))\n",
        "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
        "    tf_valid_dataset = tf.constant(valid_dataset)\n",
        "    tf_test_dataset = tf.constant(test_dataset)\n",
        "\n",
        "    # Variables.\n",
        "    weights_1 = tf.Variable(tf.truncated_normal([image_size * image_size, num_nodes]))\n",
        "    biases_1 = tf.Variable(tf.zeros([num_nodes]))\n",
        "    weights_2 = tf.Variable(tf.truncated_normal([num_nodes, num_labels]))\n",
        "    biases_2 = tf.Variable(tf.zeros([num_labels]))\n",
        "\n",
        "    # Training computation.\n",
        "    logits_1 = tf.matmul(tf_train_dataset, weights_1) + biases_1\n",
        "    relu_layer= tf.nn.relu(logits_1)\n",
        "    logits_2 = tf.matmul(relu_layer, weights_2) + biases_2\n",
        "    # Normal loss function\n",
        "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits_2, labels =tf_train_labels))\n",
        "\n",
        "# date 2020-05-25\n",
        "# tf.nn.softmax_cross_entropy_with_logits(_sentinel=None, labels=None, logits=None, dim=-1, name=None, axis=None)\n",
        "# Computes softmax cross entropy between logits and labels. (deprecated)\n",
        "# Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version. Instructions for updating:\n",
        "# Future major versions of TensorFlow will allow gradients to flow into the labels input on backprop by default.\n",
        "# See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
        "# Measures the probability error in discrete classification tasks in which the classes are mutually exclusive \n",
        "# (each entry is in exactly one class). For example, each CIFAR-10 image is labeled with one and only one label: \n",
        "# an image can be a dog or a truck, but not both.\n",
        "# **NOTE:**  While the classes are mutually exclusive, their probabilities  \n",
        "# need not be. All that is required is that each row of labels is a valid probability distribution. If they are not,\n",
        "#  the computation of the gradient will be incorrect.\n",
        "# If using exclusive labels (wherein one and only one class is true at a time), see sparse_softmax_cross_entropy_with_logits.\n",
        "# **WARNING:** This op expects unscaled logits, since it performs a `softmax`  \n",
        "# on logits internally for efficiency. Do not call this op with the output of softmax, as it will produce incorrect results.\n",
        "# A common use case is to have logits and labels of shape [batch_size, num_classes], but higher dimensions are supported, \n",
        "# with the dim argument specifying the class dimension.\n",
        "# Backpropagation will happen only into logits. To calculate a cross entropy loss that allows backpropagation into both \n",
        "# logits and labels, see tf.nn.softmax_cross_entropy_with_logits_v2.\n",
        "# **Note that to avoid confusion, it is required to pass only named arguments to  \n",
        "# this function.**\n",
        "# Args:\n",
        "#   _sentinel: Used to prevent positional parameters. Internal, do not use.  \n",
        "#   labels: Each vector along the class dimension should hold a valid  \n",
        "#     probability distribution e.g. for the case in which labels are of shape  \n",
        "\n",
        "\n",
        "    # Loss function with L2 Regularization with beta=0.01\n",
        "    regularizers = tf.nn.l2_loss(weights_1) + tf.nn.l2_loss(weights_2)\n",
        "    loss = tf.reduce_mean(loss + beta * regularizers)\n",
        "\n",
        "    # Optimizer.\n",
        "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
        "\n",
        "    # Predictions for the training\n",
        "    train_prediction = tf.nn.softmax(logits_2)\n",
        "    \n",
        "    # Predictions for validation \n",
        "    logits_1 = tf.matmul(tf_valid_dataset, weights_1) + biases_1\n",
        "    relu_layer= tf.nn.relu(logits_1)\n",
        "    logits_2 = tf.matmul(relu_layer, weights_2) + biases_2\n",
        "    \n",
        "    valid_prediction = tf.nn.softmax(logits_2)\n",
        "    \n",
        "    # Predictions for test\n",
        "    logits_1 = tf.matmul(tf_test_dataset, weights_1) + biases_1\n",
        "    relu_layer= tf.nn.relu(logits_1)\n",
        "    logits_2 = tf.matmul(relu_layer, weights_2) + biases_2\n",
        "    \n",
        "    test_prediction =  tf.nn.softmax(logits_2)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gt_w-vI-Ewz7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        },
        "outputId": "9a6b2bf9-c73e-4c52-8c17-f738d0babc81"
      },
      "source": [
        "num_steps = 3001\n",
        "\n",
        "with tf.Session(graph=graph) as session:\n",
        "    tf.initialize_all_variables().run()\n",
        "    print(\"Initialized\")\n",
        "    for step in range(num_steps):\n",
        "        # Pick an offset within the training data, which has been randomized.\n",
        "        # Note: we could use better randomization across epochs.\n",
        "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
        "        # Generate a minibatch.\n",
        "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
        "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
        "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
        "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
        "        # and the value is the numpy array to feed to it.\n",
        "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
        "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
        "        if (step % 500 == 0):\n",
        "            print(\"Minibatch loss at step {}: {}\".format(step, l))\n",
        "            print(\"Minibatch accuracy: {:.1f}\".format(accuracy(predictions, batch_labels)))\n",
        "            print(\"Validation accuracy: {:.1f}\".format(accuracy(valid_prediction.eval(), valid_labels)))\n",
        "    print(\"Test accuracy: {:.1f}\".format(accuracy(test_prediction.eval(), test_labels)))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initialized\n",
            "Minibatch loss at step 0: 3476.864013671875\n",
            "Minibatch accuracy: 12.5\n",
            "Validation accuracy: 30.5\n",
            "Minibatch loss at step 500: 21.375415802001953\n",
            "Minibatch accuracy: 79.7\n",
            "Validation accuracy: 84.6\n",
            "Minibatch loss at step 1000: 0.886101245880127\n",
            "Minibatch accuracy: 82.0\n",
            "Validation accuracy: 83.5\n",
            "Minibatch loss at step 1500: 0.6685487031936646\n",
            "Minibatch accuracy: 85.9\n",
            "Validation accuracy: 83.6\n",
            "Minibatch loss at step 2000: 0.7251509428024292\n",
            "Minibatch accuracy: 83.6\n",
            "Validation accuracy: 83.5\n",
            "Minibatch loss at step 2500: 0.6175293326377869\n",
            "Minibatch accuracy: 87.5\n",
            "Validation accuracy: 83.2\n",
            "Minibatch loss at step 3000: 0.9254697561264038\n",
            "Minibatch accuracy: 74.2\n",
            "Validation accuracy: 83.9\n",
            "Test accuracy: 90.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xgF0MZV5E_kj",
        "colab_type": "text"
      },
      "source": [
        "Problem 2\n",
        "\n",
        "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKSMCbYoFDhD",
        "colab_type": "text"
      },
      "source": [
        "Continuing from the Neural Network with L2 Regularization above"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22kJC5HrFKx6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        },
        "outputId": "bf5ab476-388d-48e7-8db9-e6aaa761ea79"
      },
      "source": [
        "num_steps = 3001\n",
        "\n",
        "train_dataset_2 = train_dataset[:500, :]\n",
        "train_labels_2 = train_labels[:500]\n",
        "\n",
        "with tf.Session(graph=graph) as session:\n",
        "    tf.initialize_all_variables().run()\n",
        "    print(\"Initialized\")\n",
        "    for step in range(num_steps):\n",
        "        # Pick an offset within the training data, which has been randomized.\n",
        "        # Note: we could use better randomization across epochs.\n",
        "        offset = (step * batch_size) % (train_labels_2.shape[0] - batch_size)\n",
        "        # Generate a minibatch.\n",
        "        batch_data = train_dataset_2[offset:(offset + batch_size), :]\n",
        "        batch_labels = train_labels_2[offset:(offset + batch_size), :]\n",
        "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
        "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
        "        # and the value is the numpy array to feed to it.\n",
        "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
        "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
        "        if (step % 500 == 0):\n",
        "            print(\"Minibatch loss at step {}: {}\".format(step, l))\n",
        "            print(\"Minibatch accuracy: {:.1f}\".format(accuracy(predictions, batch_labels)))\n",
        "            print(\"Validation accuracy: {:.1f}\".format(accuracy(valid_prediction.eval(), valid_labels)))\n",
        "    print(\"Test accuracy: {:.1f}\".format(accuracy(test_prediction.eval(), test_labels)))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initialized\n",
            "Minibatch loss at step 0: 3490.21728515625\n",
            "Minibatch accuracy: 7.8\n",
            "Validation accuracy: 29.1\n",
            "Minibatch loss at step 500: 21.103267669677734\n",
            "Minibatch accuracy: 100.0\n",
            "Validation accuracy: 77.0\n",
            "Minibatch loss at step 1000: 0.48421424627304077\n",
            "Minibatch accuracy: 100.0\n",
            "Validation accuracy: 77.2\n",
            "Minibatch loss at step 1500: 0.30161774158477783\n",
            "Minibatch accuracy: 100.0\n",
            "Validation accuracy: 77.2\n",
            "Minibatch loss at step 2000: 0.283530592918396\n",
            "Minibatch accuracy: 100.0\n",
            "Validation accuracy: 77.1\n",
            "Minibatch loss at step 2500: 0.2768358886241913\n",
            "Minibatch accuracy: 100.0\n",
            "Validation accuracy: 77.1\n",
            "Minibatch loss at step 3000: 0.27524638175964355\n",
            "Minibatch accuracy: 100.0\n",
            "Validation accuracy: 77.0\n",
            "Test accuracy: 83.9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lU1SrFuEFOgq",
        "colab_type": "text"
      },
      "source": [
        "As you can see, there's high training accuracy but low validation accuracy. There is overfitting here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJfXKNRMFRiS",
        "colab_type": "text"
      },
      "source": [
        "Problem 3\n",
        "\n",
        "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides nn.dropout() for that, but you have to make sure it's only inserted during training.\n",
        "\n",
        "What happens to our extreme overfitting case?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UyLlzh4iFVfC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "a6c0be80-1d1b-4b15-a919-ef4dba449126"
      },
      "source": [
        "num_nodes= 1024\n",
        "batch_size = 128\n",
        "beta = 0.01\n",
        "\n",
        "graph = tf.Graph()\n",
        "with graph.as_default():\n",
        "\n",
        "    # Input data. For the training data, we use a placeholder that will be fed\n",
        "    # at run time with a training minibatch.\n",
        "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size))\n",
        "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
        "    tf_valid_dataset = tf.constant(valid_dataset)\n",
        "    tf_test_dataset = tf.constant(test_dataset)\n",
        "\n",
        "    # Variables.\n",
        "    weights_1 = tf.Variable(tf.truncated_normal([image_size * image_size, num_nodes]))\n",
        "    biases_1 = tf.Variable(tf.zeros([num_nodes]))\n",
        "    weights_2 = tf.Variable(tf.truncated_normal([num_nodes, num_labels]))\n",
        "    biases_2 = tf.Variable(tf.zeros([num_labels]))\n",
        "    \n",
        "    # Training computation.\n",
        "    logits_1 = tf.matmul(tf_train_dataset, weights_1) + biases_1\n",
        "    relu_layer= tf.nn.relu(logits_1)\n",
        "    # Dropout on hidden layer: RELU layer\n",
        "    keep_prob = tf.placeholder(\"float\")\n",
        "    # WARNING:tensorflow:From <ipython-input-12-20bc49b7b7ef>:26: calling dropout (from tensorflow.python.ops.nn_ops) with \n",
        "    # keep_prob is deprecated and will be removed in a future version.\n",
        "    # Instructions for updating:\n",
        "    # Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
        "\n",
        "    relu_layer_dropout = tf.nn.dropout(relu_layer, keep_prob)\n",
        "    \n",
        "    logits_2 = tf.matmul(relu_layer_dropout, weights_2) + biases_2\n",
        "    # Normal loss function\n",
        "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits_2, labels = tf_train_labels))\n",
        "    # Loss function with L2 Regularization with beta=0.01\n",
        "    regularizers = tf.nn.l2_loss(weights_1) + tf.nn.l2_loss(weights_2)\n",
        "    loss = tf.reduce_mean(loss + beta * regularizers)\n",
        "\n",
        "    # Optimizer.\n",
        "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
        "\n",
        "    # Predictions for the training\n",
        "    train_prediction = tf.nn.softmax(logits_2)\n",
        "    \n",
        "    # Predictions for validation \n",
        "    logits_1 = tf.matmul(tf_valid_dataset, weights_1) + biases_1\n",
        "    relu_layer= tf.nn.relu(logits_1)\n",
        "    logits_2 = tf.matmul(relu_layer, weights_2) + biases_2\n",
        "    \n",
        "    valid_prediction = tf.nn.softmax(logits_2)\n",
        "    \n",
        "    # Predictions for test\n",
        "    logits_1 = tf.matmul(tf_test_dataset, weights_1) + biases_1\n",
        "    relu_layer= tf.nn.relu(logits_1)\n",
        "    logits_2 = tf.matmul(relu_layer, weights_2) + biases_2\n",
        "    \n",
        "    test_prediction =  tf.nn.softmax(logits_2)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-31-389daf2886b2>:31: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUy-A1lgFazE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        },
        "outputId": "9929aa97-4a9c-48e2-f46b-16ae26a7fa29"
      },
      "source": [
        "num_steps = 3001\n",
        "\n",
        "with tf.Session(graph=graph) as session:\n",
        "    tf.initialize_all_variables().run()\n",
        "    print(\"Initialized\")\n",
        "    for step in range(num_steps):\n",
        "        # Pick an offset within the training data, which has been randomized.\n",
        "        # Note: we could use better randomization across epochs.\n",
        "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
        "        # Generate a minibatch.\n",
        "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
        "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
        "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
        "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
        "        # and the value is the numpy array to feed to it.\n",
        "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, keep_prob : 0.5}\n",
        "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
        "        if (step % 500 == 0):\n",
        "            print(\"Minibatch loss at step {}: {}\".format(step, l))\n",
        "            print(\"Minibatch accuracy: {:.1f}\".format(accuracy(predictions, batch_labels)))\n",
        "            print(\"Validation accuracy: {:.1f}\".format(accuracy(valid_prediction.eval(), valid_labels)))\n",
        "    print(\"Test accuracy: {:.1f}\".format(accuracy(test_prediction.eval(), test_labels)))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initialized\n",
            "Minibatch loss at step 0: 3716.180908203125\n",
            "Minibatch accuracy: 8.6\n",
            "Validation accuracy: 23.4\n",
            "Minibatch loss at step 500: 21.527236938476562\n",
            "Minibatch accuracy: 78.1\n",
            "Validation accuracy: 83.7\n",
            "Minibatch loss at step 1000: 0.9519579410552979\n",
            "Minibatch accuracy: 82.0\n",
            "Validation accuracy: 82.8\n",
            "Minibatch loss at step 1500: 0.74945068359375\n",
            "Minibatch accuracy: 85.9\n",
            "Validation accuracy: 83.1\n",
            "Minibatch loss at step 2000: 0.8026576638221741\n",
            "Minibatch accuracy: 82.0\n",
            "Validation accuracy: 83.1\n",
            "Minibatch loss at step 2500: 0.6801612377166748\n",
            "Minibatch accuracy: 86.7\n",
            "Validation accuracy: 82.6\n",
            "Minibatch loss at step 3000: 0.9686069488525391\n",
            "Minibatch accuracy: 74.2\n",
            "Validation accuracy: 83.4\n",
            "Test accuracy: 89.9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f20i-4CLFeNa",
        "colab_type": "text"
      },
      "source": [
        "Extreme Overfitting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3AU3Pe2BFjQx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        },
        "outputId": "46e568df-3880-4938-e9c6-47e3e978a3c1"
      },
      "source": [
        "num_steps = 3001\n",
        "beta = 0.01\n",
        "\n",
        "train_dataset_2 = train_dataset[:500, :]\n",
        "train_labels_2 = train_labels[:500]\n",
        "\n",
        "with tf.Session(graph=graph) as session:\n",
        "    tf.initialize_all_variables().run()\n",
        "    print(\"Initialized\")\n",
        "    for step in range(num_steps):\n",
        "        # Pick an offset within the training data, which has been randomized.\n",
        "        # Note: we could use better randomization across epochs.\n",
        "        offset = (step * batch_size) % (train_labels_2.shape[0] - batch_size)\n",
        "        # Generate a minibatch.\n",
        "        batch_data = train_dataset_2[offset:(offset + batch_size), :]\n",
        "        batch_labels = train_labels_2[offset:(offset + batch_size), :]\n",
        "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
        "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
        "        # and the value is the numpy array to feed to it.\n",
        "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels,  keep_prob : 0.5}\n",
        "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
        "        if (step % 500 == 0):\n",
        "            print(\"Minibatch loss at step {}: {}\".format(step, l))\n",
        "            print(\"Minibatch accuracy: {:.1f}\".format(accuracy(predictions, batch_labels)))\n",
        "            print(\"Validation accuracy: {:.1f}\".format(accuracy(valid_prediction.eval(), valid_labels)))\n",
        "    print(\"Test accuracy: {:.1f}\".format(accuracy(test_prediction.eval(), test_labels)))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initialized\n",
            "Minibatch loss at step 0: 3625.32958984375\n",
            "Minibatch accuracy: 12.5\n",
            "Validation accuracy: 37.3\n",
            "Minibatch loss at step 500: 21.163591384887695\n",
            "Minibatch accuracy: 99.2\n",
            "Validation accuracy: 76.8\n",
            "Minibatch loss at step 1000: 0.5126768350601196\n",
            "Minibatch accuracy: 100.0\n",
            "Validation accuracy: 77.5\n",
            "Minibatch loss at step 1500: 0.33722013235092163\n",
            "Minibatch accuracy: 100.0\n",
            "Validation accuracy: 77.6\n",
            "Minibatch loss at step 2000: 0.3178301155567169\n",
            "Minibatch accuracy: 100.0\n",
            "Validation accuracy: 77.3\n",
            "Minibatch loss at step 2500: 0.30229055881500244\n",
            "Minibatch accuracy: 100.0\n",
            "Validation accuracy: 77.4\n",
            "Minibatch loss at step 3000: 0.30991989374160767\n",
            "Minibatch accuracy: 99.2\n",
            "Validation accuracy: 77.4\n",
            "Test accuracy: 84.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcn5M7kqF0wC",
        "colab_type": "text"
      },
      "source": [
        "Problem 4\n",
        "\n",
        "Try to get the best performance you can using a multi-layer model! The best \n",
        "\n",
        "reported test accuracy using a deep network is 97.1%."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjIa4gbTF3Gp",
        "colab_type": "text"
      },
      "source": [
        "One avenue you can explore is to add multiple layers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0B0H79TrF6gB",
        "colab_type": "text"
      },
      "source": [
        "Another one is to use learning rate decay:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YU-i8_zkF8n4",
        "colab_type": "text"
      },
      "source": [
        "global_step = tf.Variable(0)  # count the number of steps taken.\n",
        "\n",
        "learning_rate = tf.train.exponential_decay(0.5, global_step, ...)\n",
        "\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rT7UZSmcF_qx",
        "colab_type": "text"
      },
      "source": [
        "Model\n",
        "\n",
        "  - 5 hidden layers NN\n",
        "\n",
        "    - RELUs\n",
        "\n",
        "    - Number of nodes decrease by 50% with each hidden layer that is deeper in  \n",
        "      the neural net\n",
        "\n",
        "  - Overfitting measures\n",
        "\n",
        "    - L2 Regularization\n",
        "\n",
        "      - Learning rate (beta) with exponential decay\n",
        "\n",
        "    - Dropout\n",
        "\n",
        "  - 10,000 steps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9URp6qWGDyp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math as math"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEijxXu7GQcf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 128\n",
        "beta = 0.001\n",
        "\n",
        "hidden_nodes_1 = 1024\n",
        "hidden_nodes_2 = int(hidden_nodes_1 * 0.5)\n",
        "hidden_nodes_3 = int(hidden_nodes_1 * np.power(0.5, 2))\n",
        "hidden_nodes_4 = int(hidden_nodes_1 * np.power(0.5, 3))\n",
        "hidden_nodes_5 = int(hidden_nodes_1 * np.power(0.5, 4))\n",
        "\n",
        "graph = tf.Graph()\n",
        "with graph.as_default():\n",
        "\n",
        "    '''Input Data'''\n",
        "    # For the training data, we use a placeholder that will be fed\n",
        "    # at run time with a training minibatch.\n",
        "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size))\n",
        "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
        "    tf_valid_dataset = tf.constant(valid_dataset)\n",
        "    tf_test_dataset = tf.constant(test_dataset)\n",
        "\n",
        "    '''Variables'''\n",
        "    # Hidden RELU layer 1\n",
        "    weights_1 = tf.Variable(tf.truncated_normal([image_size * image_size, hidden_nodes_1], stddev=math.sqrt(2.0/(image_size*image_size))))\n",
        "    biases_1 = tf.Variable(tf.zeros([hidden_nodes_1]))\n",
        "\n",
        "    # Hidden RELU layer 2\n",
        "    weights_2 = tf.Variable(tf.truncated_normal([hidden_nodes_1, hidden_nodes_2], stddev=math.sqrt(2.0/hidden_nodes_1)))\n",
        "    biases_2 = tf.Variable(tf.zeros([hidden_nodes_2]))\n",
        "    \n",
        "    # Hidden RELU layer 3\n",
        "    weights_3 = tf.Variable(tf.truncated_normal([hidden_nodes_2, hidden_nodes_3], stddev=math.sqrt(2.0/hidden_nodes_2)))\n",
        "    biases_3 = tf.Variable(tf.zeros([hidden_nodes_3]))\n",
        "    \n",
        "    # Hidden RELU layer 4\n",
        "    weights_4 = tf.Variable(tf.truncated_normal([hidden_nodes_3, hidden_nodes_4], stddev=math.sqrt(2.0/hidden_nodes_3)))\n",
        "    biases_4 = tf.Variable(tf.zeros([hidden_nodes_4]))\n",
        "    \n",
        "    # Hidden RELU layer 5\n",
        "    weights_5 = tf.Variable(tf.truncated_normal([hidden_nodes_4, hidden_nodes_5], stddev=math.sqrt(2.0/hidden_nodes_4)))\n",
        "    biases_5 = tf.Variable(tf.zeros([hidden_nodes_5]))\n",
        "    \n",
        "    # Output layer\n",
        "    weights_6 = tf.Variable(tf.truncated_normal([hidden_nodes_5, num_labels], stddev=math.sqrt(2.0/hidden_nodes_5)))\n",
        "    biases_6 = tf.Variable(tf.zeros([num_labels]))\n",
        "    \n",
        "    '''Training computation'''\n",
        "    \n",
        "    # Hidden RELU layer 1\n",
        "    logits_1 = tf.matmul(tf_train_dataset, weights_1) + biases_1\n",
        "    hidden_layer_1 = tf.nn.relu(logits_1)\n",
        "    # Dropout on hidden layer: RELU layer\n",
        "    keep_prob = tf.placeholder(\"float\")\n",
        "    hidden_layer_1_dropout = tf.nn.dropout(hidden_layer_1, keep_prob)\n",
        "    \n",
        "    \n",
        "    # Hidden RELU layer 2\n",
        "    logits_2 = tf.matmul(hidden_layer_1_dropout, weights_2) + biases_2\n",
        "    hidden_layer_2 = tf.nn.relu(logits_2)\n",
        "    # Dropout on hidden layer: RELU layer\n",
        "    hidden_layer_2_dropout = tf.nn.dropout(hidden_layer_2, keep_prob)\n",
        "    \n",
        "    # Hidden RELU layer 3\n",
        "    logits_3 = tf.matmul(hidden_layer_2_dropout, weights_3) + biases_3\n",
        "    hidden_layer_3 = tf.nn.relu(logits_3)\n",
        "    # Dropout on hidden layer: RELU layer\n",
        "    hidden_layer_3_dropout = tf.nn.dropout(hidden_layer_3, keep_prob)\n",
        "    \n",
        "    # Hidden RELU layer 4\n",
        "    logits_4 = tf.matmul(hidden_layer_3_dropout, weights_4) + biases_4\n",
        "    hidden_layer_4 = tf.nn.relu(logits_4)\n",
        "    # Dropout on hidden layer: RELU layer\n",
        "\n",
        "    hidden_layer_4_dropout = tf.nn.dropout(hidden_layer_4, keep_prob)\n",
        "    \n",
        "    # Hidden RELU layer 5\n",
        "    logits_5 = tf.matmul(hidden_layer_4_dropout, weights_5) + biases_5\n",
        "    hidden_layer_5 = tf.nn.relu(logits_5)\n",
        "    # Dropout on hidden layer: RELU layer\n",
        "    hidden_layer_5_dropout = tf.nn.dropout(hidden_layer_5, keep_prob)\n",
        "    \n",
        "    # Output layer\n",
        "    logits_6 = tf.matmul(hidden_layer_5_dropout, weights_6) + biases_6 \n",
        "    \n",
        "    # Normal loss function\n",
        "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits_6, labels = tf_train_labels))\n",
        "    # Loss function with L2 Regularization with decaying learning rate beta=0.5\n",
        "    regularizers = tf.nn.l2_loss(weights_1) + tf.nn.l2_loss(weights_2) + \\\n",
        "                   tf.nn.l2_loss(weights_3) + tf.nn.l2_loss(weights_4) + \\\n",
        "                   tf.nn.l2_loss(weights_5) + tf.nn.l2_loss(weights_6)\n",
        "    loss = tf.reduce_mean(loss + beta * regularizers)\n",
        "\n",
        "    '''Optimizer'''\n",
        "    # Decaying learning rate\n",
        "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
        "    start_learning_rate = 0.5\n",
        "    learning_rate = tf.train.exponential_decay(start_learning_rate, global_step, 100000, 0.96, staircase=True)\n",
        "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
        "    \n",
        "    # Predictions for the training\n",
        "    train_prediction = tf.nn.softmax(logits_6)\n",
        "    \n",
        "    # Predictions for validation \n",
        "    valid_logits_1 = tf.matmul(tf_valid_dataset, weights_1) + biases_1\n",
        "    valid_relu_1 = tf.nn.relu(valid_logits_1)\n",
        "    \n",
        "    valid_logits_2 = tf.matmul(valid_relu_1, weights_2) + biases_2\n",
        "    valid_relu_2 = tf.nn.relu(valid_logits_2)\n",
        "    \n",
        "    valid_logits_3 = tf.matmul(valid_relu_2, weights_3) + biases_3\n",
        "    valid_relu_3 = tf.nn.relu(valid_logits_3)\n",
        "    \n",
        "    valid_logits_4 = tf.matmul(valid_relu_3, weights_4) + biases_4\n",
        "    valid_relu_4 = tf.nn.relu(valid_logits_4)\n",
        "    \n",
        "    valid_logits_5 = tf.matmul(valid_relu_4, weights_5) + biases_5\n",
        "    valid_relu_5 = tf.nn.relu(valid_logits_5)\n",
        "    \n",
        "    valid_logits_6 = tf.matmul(valid_relu_5, weights_6) + biases_6\n",
        "    \n",
        "    valid_prediction = tf.nn.softmax(valid_logits_6)\n",
        "    \n",
        "    # Predictions for test\n",
        "    test_logits_1 = tf.matmul(tf_test_dataset, weights_1) + biases_1\n",
        "    test_relu_1 = tf.nn.relu(test_logits_1)\n",
        "    \n",
        "    test_logits_2 = tf.matmul(test_relu_1, weights_2) + biases_2\n",
        "    test_relu_2 = tf.nn.relu(test_logits_2)\n",
        "    \n",
        "    test_logits_3 = tf.matmul(test_relu_2, weights_3) + biases_3\n",
        "    test_relu_3 = tf.nn.relu(test_logits_3)\n",
        "    \n",
        "    test_logits_4 = tf.matmul(test_relu_3, weights_4) + biases_4\n",
        "    test_relu_4 = tf.nn.relu(test_logits_4)\n",
        "    \n",
        "    test_logits_5 = tf.matmul(test_relu_4, weights_5) + biases_5\n",
        "    test_relu_5 = tf.nn.relu(test_logits_5)\n",
        "    \n",
        "    test_logits_6 = tf.matmul(test_relu_5, weights_6) + biases_6\n",
        "    \n",
        "    test_prediction = tf.nn.softmax(test_logits_6)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5PKmMhQGUmI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f532e8be-8651-4133-8bba-1163100f15e1"
      },
      "source": [
        "num_steps = 15000            ### step ນີ້ລັນຄັ້ງທຳອິດ 733.411ວິນາທີ = 12.22ນາທີ\n",
        "\n",
        "with tf.Session(graph=graph) as session:\n",
        "    tf.initialize_all_variables().run()\n",
        "    print(\"Initialized\")\n",
        "    for step in range(num_steps):\n",
        "        # Pick an offset within the training data, which has been randomized.\n",
        "        # Note: we could use better randomization across epochs.\n",
        "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
        "        # Generate a minibatch.\n",
        "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
        "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
        "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
        "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
        "        # and the value is the numpy array to feed to it.\n",
        "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, keep_prob : 0.5}\n",
        "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
        "        if (step % 500 == 0):\n",
        "            print(\"Minibatch loss at step {}: {}\".format(step, l))\n",
        "            print(\"Minibatch accuracy: {:.1f}\".format(accuracy(predictions, batch_labels)))\n",
        "            print(\"Validation accuracy: {:.1f}\".format(accuracy(valid_prediction.eval(), valid_labels)))\n",
        "    print(\"Test accuracy: {:.1f}\".format(accuracy(test_prediction.eval(), test_labels)))\n",
        "\n",
        "### 200524 ກອບປີ້ມາແລ້ວມື້ນີ້\n",
        "### 200525 ລັນຜ່ານຄັ້ງທຳອິດ ### step ນີ້ລັນຄັ້ງທຳອິດ 733.411ວິນາທີ = 12.22ນາທີ"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initialized\n",
            "Minibatch loss at step 0: 4.613100051879883\n",
            "Minibatch accuracy: 10.2\n",
            "Validation accuracy: 10.7\n",
            "Minibatch loss at step 500: 2.0181736946105957\n",
            "Minibatch accuracy: 74.2\n",
            "Validation accuracy: 82.7\n",
            "Minibatch loss at step 1000: 1.3136873245239258\n",
            "Minibatch accuracy: 82.8\n",
            "Validation accuracy: 83.7\n",
            "Minibatch loss at step 1500: 1.088275671005249\n",
            "Minibatch accuracy: 86.7\n",
            "Validation accuracy: 84.3\n",
            "Minibatch loss at step 2000: 1.0702412128448486\n",
            "Minibatch accuracy: 81.2\n",
            "Validation accuracy: 84.4\n",
            "Minibatch loss at step 2500: 0.8112349510192871\n",
            "Minibatch accuracy: 89.1\n",
            "Validation accuracy: 85.4\n",
            "Minibatch loss at step 3000: 1.075466513633728\n",
            "Minibatch accuracy: 77.3\n",
            "Validation accuracy: 85.6\n",
            "Minibatch loss at step 3500: 0.7931531667709351\n",
            "Minibatch accuracy: 82.0\n",
            "Validation accuracy: 85.7\n",
            "Minibatch loss at step 4000: 0.7554179430007935\n",
            "Minibatch accuracy: 88.3\n",
            "Validation accuracy: 85.6\n",
            "Minibatch loss at step 4500: 0.9653371572494507\n",
            "Minibatch accuracy: 80.5\n",
            "Validation accuracy: 86.0\n",
            "Minibatch loss at step 5000: 0.8803838491439819\n",
            "Minibatch accuracy: 82.0\n",
            "Validation accuracy: 85.6\n",
            "Minibatch loss at step 5500: 0.7801551818847656\n",
            "Minibatch accuracy: 84.4\n",
            "Validation accuracy: 86.0\n",
            "Minibatch loss at step 6000: 0.7484075427055359\n",
            "Minibatch accuracy: 85.2\n",
            "Validation accuracy: 86.3\n",
            "Minibatch loss at step 6500: 0.7773123979568481\n",
            "Minibatch accuracy: 85.2\n",
            "Validation accuracy: 86.4\n",
            "Minibatch loss at step 7000: 0.8596134185791016\n",
            "Minibatch accuracy: 84.4\n",
            "Validation accuracy: 86.5\n",
            "Minibatch loss at step 7500: 0.8259977102279663\n",
            "Minibatch accuracy: 82.8\n",
            "Validation accuracy: 86.1\n",
            "Minibatch loss at step 8000: 0.8584358096122742\n",
            "Minibatch accuracy: 85.2\n",
            "Validation accuracy: 86.5\n",
            "Minibatch loss at step 8500: 0.8810609579086304\n",
            "Minibatch accuracy: 82.8\n",
            "Validation accuracy: 86.5\n",
            "Minibatch loss at step 9000: 0.8354507684707642\n",
            "Minibatch accuracy: 82.8\n",
            "Validation accuracy: 86.5\n",
            "Minibatch loss at step 9500: 0.7959256768226624\n",
            "Minibatch accuracy: 83.6\n",
            "Validation accuracy: 86.0\n",
            "Minibatch loss at step 10000: 0.5920791625976562\n",
            "Minibatch accuracy: 89.1\n",
            "Validation accuracy: 86.1\n",
            "Minibatch loss at step 10500: 0.8381997346878052\n",
            "Minibatch accuracy: 84.4\n",
            "Validation accuracy: 86.3\n",
            "Minibatch loss at step 11000: 0.9365146160125732\n",
            "Minibatch accuracy: 78.1\n",
            "Validation accuracy: 85.2\n",
            "Minibatch loss at step 11500: 0.8482808470726013\n",
            "Minibatch accuracy: 88.3\n",
            "Validation accuracy: 87.1\n",
            "Minibatch loss at step 12000: 0.9247506856918335\n",
            "Minibatch accuracy: 78.9\n",
            "Validation accuracy: 86.3\n",
            "Minibatch loss at step 12500: 0.7381875514984131\n",
            "Minibatch accuracy: 85.2\n",
            "Validation accuracy: 86.0\n",
            "Minibatch loss at step 13000: 0.6930271983146667\n",
            "Minibatch accuracy: 88.3\n",
            "Validation accuracy: 86.4\n",
            "Minibatch loss at step 13500: 0.6624893546104431\n",
            "Minibatch accuracy: 88.3\n",
            "Validation accuracy: 86.4\n",
            "Minibatch loss at step 14000: 0.7029365301132202\n",
            "Minibatch accuracy: 85.2\n",
            "Validation accuracy: 86.2\n",
            "Minibatch loss at step 14500: 0.6696926355361938\n",
            "Minibatch accuracy: 85.9\n",
            "Validation accuracy: 86.9\n",
            "Test accuracy: 93.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0RL_Pedg44J",
        "colab_type": "text"
      },
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nm2s9fl7g2Hs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "0d95a045-4f32-4e2e-e87d-4f073296721b"
      },
      "source": [
        "#import numpy as np\n",
        "#import tensorflow as tf\n",
        "\n",
        "b = np.arange(6)\n",
        "b[3] = 9\n",
        "print(b)\n",
        "np.argmax(b)\n"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 1 2 9 4 5]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    }
  ]
}